{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pra conseguir algum erro util da porra do torch\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0666d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, get_constant_schedule_with_warmup, BertForSequenceClassification\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.skmultilearn.model_selection.iterative_stratification import iterative_train_test_split, IterativeStratification\n",
    "from sklearn.metrics import classification_report, average_precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71705c4",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b5fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4, min_delta=0,is_loss=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.is_loss = is_loss\n",
    "        if is_loss:\n",
    "            self.min_validation_loss = np.inf\n",
    "        else:\n",
    "            self.min_validation_loss = -np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if self.is_loss:\n",
    "            if validation_loss < self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW LOWEST LOSS \",self.min_validation_loss)\n",
    "            elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            if validation_loss > self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW HIGHEST SCORE \",self.min_validation_loss)\n",
    "            elif validation_loss <= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86eb69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f73e5",
   "metadata": {},
   "source": [
    "# Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276a9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 2023\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "  device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a560d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_train_epochs = 20\n",
    "print_each_n_step = 100\n",
    "batch_size= 16\n",
    "warmup_proportion = 0.2\n",
    "apply_scheduler = True\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5249173",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff7194",
   "metadata": {},
   "source": [
    "## Tarefa primária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1017af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    df['Alteração na eficiência/funcionalidade'] = (df['Alteração na eficiência/funcionalidade'] == 1) | (df[\"Alteração da funcionalidade\"] == 1) | (df[\"Alteração na eficiência\"] == 1)\n",
    "    df = df.drop([\"Postagem com possível perfil depressivo\",\"Alteração na eficiência\",\n",
    "         \"Alteração da funcionalidade\",\"*\",'Agitação/inquietação','Sintoma obsessivo e compulsivo','Déficit de atenção/Memória',\n",
    "              'Perda/Diminuição do prazer/ Perda/Diminuição da libido'],axis=1)\n",
    "    df['Neutro'] =  df.iloc[:,1:].sum(axis=1) == 0\n",
    "    df = df.replace({True:1,False:0})\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9cea857",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_data(pd.read_csv(\"data/segredos_sentenças_multitask_train_clean.csv\",index_col=0))\n",
    "test_df = process_data(pd.read_csv(\"data/segredos_sentenças_multitask_test_clean.csv\",index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symptom_num = train_df.iloc[:,1:].shape[1]\n",
    "target_names_primary = list(train_df.iloc[:,1:].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c129b",
   "metadata": {},
   "source": [
    "## Tarefa auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c02ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_emotions_path = \"data/goemotions\"\n",
    "\n",
    "auxiliary_train = pd.read_csv(f\"{go_emotions_path}/train.tsv\",sep='\\t')\n",
    "auxiliary_val = pd.read_csv(f\"{go_emotions_path}/dev.tsv\",sep='\\t')\n",
    "\n",
    "auxiliary_test = pd.read_csv(f\"{go_emotions_path}/test.tsv\",sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23012bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label_encoding(row):\n",
    "    #mudar encoding para ser multirrótulo (preferível para o cálculo da entropia)\n",
    "    labels = row['labels'].replace(\" \",\"\").split(\",\")\n",
    "    new_row = {}\n",
    "    for emotion in emotion_dict:\n",
    "        if str(emotion_dict[emotion]) in labels:\n",
    "            new_row[emotion] = True\n",
    "        else:\n",
    "            new_row[emotion] = False\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c6eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {\n",
    "    \"admiração\": 0,\n",
    "    \"diversão\": 1,\n",
    "    \"raiva\": 2,\n",
    "    \"aborrecimento\": 3,\n",
    "    \"aprovação\": 4,\n",
    "    \"zelo\": 5,\n",
    "    \"confusão\": 6,\n",
    "    \"curiosidade\": 7,\n",
    "    \"desejo\": 8,\n",
    "    \"decepção\": 9,\n",
    "    \"desaprovação\": 10,\n",
    "    \"nojo\": 11,\n",
    "    \"constrangimento\": 12,\n",
    "    \"entusiasmo\": 13,\n",
    "    \"medo\": 14,\n",
    "    \"gratidão\": 15,\n",
    "    \"luto\": 16,\n",
    "    \"alegria\": 17,\n",
    "    \"amor\": 18,\n",
    "    \"nervosismo\": 19,\n",
    "    \"otimismo\": 20,\n",
    "    \"orgulho\": 21,\n",
    "    \"percepção\": 22,\n",
    "    \"alívio\": 23,\n",
    "    \"remorso\": 24,\n",
    "    \"tristeza\": 25,\n",
    "    \"surpresa\": 26,\n",
    "    \"neutro\": 27\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd40c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels_test = pd.DataFrame(list(auxiliary_test.apply(change_label_encoding,axis=1)))\n",
    "emotion_labels_train = pd.DataFrame(list(auxiliary_train.apply(change_label_encoding,axis=1)))\n",
    "emotion_labels_val = pd.DataFrame(list(auxiliary_val.apply(change_label_encoding,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "358db423",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_num = len(emotion_dict)\n",
    "target_names_auxiliary = list(auxiliary_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a14f21",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d05452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(torch.nn.Module):\n",
    "    def __init__(self,mixup_alpha=1):\n",
    "        \n",
    "        super(Mixup,self).__init__()\n",
    "    \n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "        \n",
    "    def mixup(self,batch_ids,batch_labels,alpha=1):\n",
    "        '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = batch_ids.size()[0]\n",
    "\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "\n",
    "        mixed_x = lam * batch_ids + (1 - lam) * batch_ids[index, :]\n",
    "        #using bigger input mask\n",
    "        #mixed_masks = batch_masks | batch_masks[index,:]\n",
    "        y_a, y_b = batch_labels, batch_labels[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "    def forward(self,outputs,labels):\n",
    "       \n",
    "        if self.training:\n",
    "            outputs, labels_a, labels_b, lamb = self.mixup(outputs,labels,self.mixup_alpha)\n",
    "\n",
    "            return outputs,labels_a,labels_b,lamb\n",
    "        else:\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2988c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSoftSharingLayer(nn.Module):\n",
    "    def __init__(self,encoders):\n",
    "        super(BertSoftSharingLayer,self).__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.n_tasks = len(encoders)\n",
    "        #linear combination weights between tasks\n",
    "        #self.alpha = nn.Parameter(torch.eye(len(encoders),len(encoders),requires_grad=True))\n",
    "        #self.alpha = nn.Parameter(torch.full((len(encoders),len(encoders)),0.5,requires_grad=True))\n",
    "        \n",
    "        self.alpha_prim = nn.Parameter(torch.ones(1,requires_grad=True))\n",
    "        self.beta_prim = nn.Parameter(torch.zeros(1,requires_grad=True))\n",
    "        \n",
    "        self.alpha_aux = nn.Parameter(torch.ones(1,requires_grad=True))\n",
    "        self.beta_aux = nn.Parameter(torch.zeros(1,requires_grad=True))\n",
    "        \n",
    "        self.alphas = [self.alpha_prim,self.alpha_aux]\n",
    "        self.betas = [self.beta_prim,self.beta_aux]\n",
    "\n",
    "    def prepare_for_task(self,task_idx):\n",
    "        #notar que os alphas não foram tocados. Estes são sempre treinados\n",
    "        for n in range(self.n_tasks):\n",
    "            if n != task_idx:\n",
    "                for param in self.encoders[n].parameters():\n",
    "                    param.requires_grad=False\n",
    "                    self.alphas[n].requires_grad=False\n",
    "                    self.betas[n].requires_grad=False\n",
    "            else:\n",
    "                for param in self.encoders[n].parameters():\n",
    "                    param.requires_grad=True\n",
    "                    self.alphas[n].requires_grad=True\n",
    "                    self.betas[n].requires_grad=True\n",
    "\n",
    "        \n",
    "    def forward(self,input_arr,attention_mask):\n",
    "        outputs = []\n",
    "        for i,encoder in enumerate(self.encoders):\n",
    "            outputs.append(encoder(input_arr[i],attention_mask = attention_mask)[0])\n",
    "        \n",
    "        outputs[0] = outputs[0] * self.alpha_prim + outputs[1] * self.beta_prim\n",
    "        outputs[1] = outputs[1] * self.alpha_aux + outputs[0] * self.beta_aux\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "\n",
    "        #linear_combination = torch.matmul(self.alpha,outputs.reshape(self.n_tasks,torch.prod(torch.tensor(outputs.size()[1:]))))\n",
    "        #return linear_combination.reshape(outputs.size())\n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e93b3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSharedParametersModel(nn.Module):\n",
    "    def __init__(self,models,attention_n,hidden_size,num_labels,dropout_rate=0.1,mixup_alphas=[1,0],mixup_layer=12):\n",
    "        super(BertSharedParametersModel,self).__init__()\n",
    "        self.n_tasks = len(models)\n",
    "        self.embeddings = nn.ModuleList([model.bert.embeddings for model in models])\n",
    "        soft_sharing_layers = []\n",
    "        #assumindo que os modelos têm o mesmo n° de camadas de atenção\n",
    "        for i in range(attention_n):\n",
    "            attention_layers = [model.bert.encoder.layer[i] for model in models]\n",
    "            soft_sharing_layers.append(BertSoftSharingLayer(attention_layers))\n",
    "        self.soft_sharing_layers = nn.ModuleList(soft_sharing_layers)\n",
    "        self.poolers = nn.ModuleList([model.bert.pooler for model in models])\n",
    "        self.dropouts = nn.ModuleList([model.dropout for model in models])\n",
    "        self.classification_heads = nn.ModuleList([model.classifier for model in models])\n",
    "        self.mixups = nn.ModuleList([Mixup(alpha) for alpha in mixup_alphas])\n",
    "        self.mixup_layer = mixup_layer\n",
    "        for i,_ in enumerate(self.dropouts):\n",
    "            self.dropouts.p = dropout_rate\n",
    "            \n",
    "            \n",
    "    def prepare_for_task(self,task_idx):\n",
    "        for n in range(self.n_tasks):\n",
    "            if n == task_idx:\n",
    "                for param in self.embeddings[n].parameters():\n",
    "                    param.requires_grad= True\n",
    "                \n",
    "                for i in range (len(self.soft_sharing_layers)):\n",
    "                    self.soft_sharing_layers[i].prepare_for_task(task_idx)\n",
    "                \n",
    "                for param in self.poolers[n].parameters():\n",
    "                    param.requires_grad= True\n",
    "                \n",
    "                for param in self.classification_heads[n].parameters():\n",
    "                    param_requires_grad = True\n",
    "            else:\n",
    "                \n",
    "                for param in self.embeddings[n].parameters():\n",
    "                    param.requires_grad= False\n",
    "                \n",
    "                for param in self.poolers[n].parameters():\n",
    "                    param.requires_grad= False\n",
    "                \n",
    "                for param in self.classification_heads[n].parameters():\n",
    "                    param_requires_grad = False\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self,input_arr,attention_mask,labels,task_idx):\n",
    "        \n",
    "        self.prepare_for_task(task_idx)\n",
    "\n",
    "        \n",
    "        \n",
    "        extended_attention_mask: extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float)  \n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(torch.float).min\n",
    "        outputs = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "           \n",
    "            outputs.append(embedding(input_arr))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        \n",
    "\n",
    "        for i,sharing_layer in enumerate(self.soft_sharing_layers):\n",
    "            outputs = sharing_layer(outputs,attention_mask=extended_attention_mask)\n",
    "            if i == (self.mixup_layer-1) and self.training:\n",
    "                outputs_cur_task,labels_a,labels_b,lamb = self.mixups[task_idx](outputs[task_idx],labels)\n",
    "                outputs[task_idx] = outputs_cur_task\n",
    "        outputs = self.poolers[task_idx](outputs[task_idx])\n",
    "        \n",
    "        \n",
    "        outputs = self.dropouts[task_idx](outputs)\n",
    "            \n",
    "\n",
    "        outputs = self.classification_heads[task_idx](outputs)\n",
    "        if self.training:\n",
    "            return outputs,labels_a,labels_b,lamb\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a558e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_model.prepare_for_task(5)\n",
    "#for param in combined_model.soft_sharing_layers[2].encoders[0].parameters():\n",
    "#    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f15c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#primary_model_path = \"models/BERT baselines\"\n",
    "primary_model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "#auxiliary_model_path = \"models/go_emotions\"\n",
    "auxiliary_model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "tokenizer_path = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacde06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "primary_model = BertForSequenceClassification.from_pretrained(primary_model_path,num_labels=symptom_num)\n",
    "auxiliary_model = BertForSequenceClassification.from_pretrained(auxiliary_model_path,num_labels=emotion_num)\n",
    "\n",
    "primary_config = AutoConfig.from_pretrained(primary_model_path)\n",
    "auxiliary_config = AutoConfig.from_pretrained(auxiliary_model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d17069",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert primary_config.num_hidden_layers == auxiliary_config.num_hidden_layers\n",
    "combined_model = BertSharedParametersModel([primary_model,auxiliary_model],primary_config.num_hidden_layers,\n",
    "                                          primary_config.hidden_size,[symptom_num,emotion_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc339abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "├─ModuleList: 1-1                                       --\n",
      "|    └─BertEmbeddings: 2-1                              --\n",
      "|    |    └─Embedding: 3-1                              22,881,792\n",
      "|    |    └─Embedding: 3-2                              393,216\n",
      "|    |    └─Embedding: 3-3                              1,536\n",
      "|    |    └─LayerNorm: 3-4                              1,536\n",
      "|    |    └─Dropout: 3-5                                --\n",
      "|    └─BertEmbeddings: 2-2                              --\n",
      "|    |    └─Embedding: 3-6                              22,881,792\n",
      "|    |    └─Embedding: 3-7                              393,216\n",
      "|    |    └─Embedding: 3-8                              1,536\n",
      "|    |    └─LayerNorm: 3-9                              1,536\n",
      "|    |    └─Dropout: 3-10                               --\n",
      "├─ModuleList: 1-2                                       --\n",
      "|    └─BertSoftSharingLayer: 2-3                        --\n",
      "|    |    └─ModuleList: 3-11                            --\n",
      "|    |    |    └─BertLayer: 4-1                         --\n",
      "|    |    |    |    └─BertAttention: 5-1                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-2             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-3                   2,361,600\n",
      "|    |    |    └─BertLayer: 4-2                         --\n",
      "|    |    |    |    └─BertAttention: 5-4                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-5             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-6                   2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-4                        --\n",
      "|    |    └─ModuleList: 3-12                            --\n",
      "|    |    |    └─BertLayer: 4-3                         --\n",
      "|    |    |    |    └─BertAttention: 5-7                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-8             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-9                   2,361,600\n",
      "|    |    |    └─BertLayer: 4-4                         --\n",
      "|    |    |    |    └─BertAttention: 5-10               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-11            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-12                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-5                        --\n",
      "|    |    └─ModuleList: 3-13                            --\n",
      "|    |    |    └─BertLayer: 4-5                         --\n",
      "|    |    |    |    └─BertAttention: 5-13               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-14            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-15                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-6                         --\n",
      "|    |    |    |    └─BertAttention: 5-16               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-17            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-18                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-6                        --\n",
      "|    |    └─ModuleList: 3-14                            --\n",
      "|    |    |    └─BertLayer: 4-7                         --\n",
      "|    |    |    |    └─BertAttention: 5-19               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-20            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-21                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-8                         --\n",
      "|    |    |    |    └─BertAttention: 5-22               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-23            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-24                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-7                        --\n",
      "|    |    └─ModuleList: 3-15                            --\n",
      "|    |    |    └─BertLayer: 4-9                         --\n",
      "|    |    |    |    └─BertAttention: 5-25               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-26            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-27                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-10                        --\n",
      "|    |    |    |    └─BertAttention: 5-28               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-29            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-30                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-8                        --\n",
      "|    |    └─ModuleList: 3-16                            --\n",
      "|    |    |    └─BertLayer: 4-11                        --\n",
      "|    |    |    |    └─BertAttention: 5-31               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-32            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-33                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-12                        --\n",
      "|    |    |    |    └─BertAttention: 5-34               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-35            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-36                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-9                        --\n",
      "|    |    └─ModuleList: 3-17                            --\n",
      "|    |    |    └─BertLayer: 4-13                        --\n",
      "|    |    |    |    └─BertAttention: 5-37               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-38            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-39                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-14                        --\n",
      "|    |    |    |    └─BertAttention: 5-40               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-41            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-42                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-10                       --\n",
      "|    |    └─ModuleList: 3-18                            --\n",
      "|    |    |    └─BertLayer: 4-15                        --\n",
      "|    |    |    |    └─BertAttention: 5-43               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-44            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-45                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-16                        --\n",
      "|    |    |    |    └─BertAttention: 5-46               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-47            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-48                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-11                       --\n",
      "|    |    └─ModuleList: 3-19                            --\n",
      "|    |    |    └─BertLayer: 4-17                        --\n",
      "|    |    |    |    └─BertAttention: 5-49               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-50            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-51                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-18                        --\n",
      "|    |    |    |    └─BertAttention: 5-52               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-53            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-54                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-12                       --\n",
      "|    |    └─ModuleList: 3-20                            --\n",
      "|    |    |    └─BertLayer: 4-19                        --\n",
      "|    |    |    |    └─BertAttention: 5-55               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-56            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-57                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-20                        --\n",
      "|    |    |    |    └─BertAttention: 5-58               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-59            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-60                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-13                       --\n",
      "|    |    └─ModuleList: 3-21                            --\n",
      "|    |    |    └─BertLayer: 4-21                        --\n",
      "|    |    |    |    └─BertAttention: 5-61               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-62            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-63                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-22                        --\n",
      "|    |    |    |    └─BertAttention: 5-64               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-65            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-66                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-14                       --\n",
      "|    |    └─ModuleList: 3-22                            --\n",
      "|    |    |    └─BertLayer: 4-23                        --\n",
      "|    |    |    |    └─BertAttention: 5-67               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-68            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-69                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-24                        --\n",
      "|    |    |    |    └─BertAttention: 5-70               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-71            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-72                  2,361,600\n",
      "├─ModuleList: 1-3                                       --\n",
      "|    └─BertPooler: 2-15                                 --\n",
      "|    |    └─Linear: 3-23                                590,592\n",
      "|    |    └─Tanh: 3-24                                  --\n",
      "|    └─BertPooler: 2-16                                 --\n",
      "|    |    └─Linear: 3-25                                590,592\n",
      "|    |    └─Tanh: 3-26                                  --\n",
      "├─ModuleList: 1-4                                       --\n",
      "|    └─Dropout: 2-17                                    --\n",
      "|    └─Dropout: 2-18                                    --\n",
      "├─ModuleList: 1-5                                       --\n",
      "|    └─Linear: 2-19                                     14,611\n",
      "|    └─Linear: 2-20                                     21,532\n",
      "├─ModuleList: 1-6                                       --\n",
      "|    └─Mixup: 2-21                                      --\n",
      "|    └─Mixup: 2-22                                      --\n",
      "================================================================================\n",
      "Total params: 217,882,415\n",
      "Trainable params: 217,882,415\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "├─ModuleList: 1-1                                       --\n",
       "|    └─BertEmbeddings: 2-1                              --\n",
       "|    |    └─Embedding: 3-1                              22,881,792\n",
       "|    |    └─Embedding: 3-2                              393,216\n",
       "|    |    └─Embedding: 3-3                              1,536\n",
       "|    |    └─LayerNorm: 3-4                              1,536\n",
       "|    |    └─Dropout: 3-5                                --\n",
       "|    └─BertEmbeddings: 2-2                              --\n",
       "|    |    └─Embedding: 3-6                              22,881,792\n",
       "|    |    └─Embedding: 3-7                              393,216\n",
       "|    |    └─Embedding: 3-8                              1,536\n",
       "|    |    └─LayerNorm: 3-9                              1,536\n",
       "|    |    └─Dropout: 3-10                               --\n",
       "├─ModuleList: 1-2                                       --\n",
       "|    └─BertSoftSharingLayer: 2-3                        --\n",
       "|    |    └─ModuleList: 3-11                            --\n",
       "|    |    |    └─BertLayer: 4-1                         --\n",
       "|    |    |    |    └─BertAttention: 5-1                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-2             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-3                   2,361,600\n",
       "|    |    |    └─BertLayer: 4-2                         --\n",
       "|    |    |    |    └─BertAttention: 5-4                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-5             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-6                   2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-4                        --\n",
       "|    |    └─ModuleList: 3-12                            --\n",
       "|    |    |    └─BertLayer: 4-3                         --\n",
       "|    |    |    |    └─BertAttention: 5-7                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-8             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-9                   2,361,600\n",
       "|    |    |    └─BertLayer: 4-4                         --\n",
       "|    |    |    |    └─BertAttention: 5-10               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-11            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-12                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-5                        --\n",
       "|    |    └─ModuleList: 3-13                            --\n",
       "|    |    |    └─BertLayer: 4-5                         --\n",
       "|    |    |    |    └─BertAttention: 5-13               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-14            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-15                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-6                         --\n",
       "|    |    |    |    └─BertAttention: 5-16               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-17            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-18                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-6                        --\n",
       "|    |    └─ModuleList: 3-14                            --\n",
       "|    |    |    └─BertLayer: 4-7                         --\n",
       "|    |    |    |    └─BertAttention: 5-19               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-20            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-21                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-8                         --\n",
       "|    |    |    |    └─BertAttention: 5-22               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-23            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-24                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-7                        --\n",
       "|    |    └─ModuleList: 3-15                            --\n",
       "|    |    |    └─BertLayer: 4-9                         --\n",
       "|    |    |    |    └─BertAttention: 5-25               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-26            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-27                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-10                        --\n",
       "|    |    |    |    └─BertAttention: 5-28               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-29            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-30                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-8                        --\n",
       "|    |    └─ModuleList: 3-16                            --\n",
       "|    |    |    └─BertLayer: 4-11                        --\n",
       "|    |    |    |    └─BertAttention: 5-31               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-32            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-33                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-12                        --\n",
       "|    |    |    |    └─BertAttention: 5-34               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-35            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-36                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-9                        --\n",
       "|    |    └─ModuleList: 3-17                            --\n",
       "|    |    |    └─BertLayer: 4-13                        --\n",
       "|    |    |    |    └─BertAttention: 5-37               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-38            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-39                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-14                        --\n",
       "|    |    |    |    └─BertAttention: 5-40               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-41            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-42                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-10                       --\n",
       "|    |    └─ModuleList: 3-18                            --\n",
       "|    |    |    └─BertLayer: 4-15                        --\n",
       "|    |    |    |    └─BertAttention: 5-43               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-44            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-45                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-16                        --\n",
       "|    |    |    |    └─BertAttention: 5-46               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-47            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-48                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-11                       --\n",
       "|    |    └─ModuleList: 3-19                            --\n",
       "|    |    |    └─BertLayer: 4-17                        --\n",
       "|    |    |    |    └─BertAttention: 5-49               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-50            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-51                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-18                        --\n",
       "|    |    |    |    └─BertAttention: 5-52               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-53            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-54                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-12                       --\n",
       "|    |    └─ModuleList: 3-20                            --\n",
       "|    |    |    └─BertLayer: 4-19                        --\n",
       "|    |    |    |    └─BertAttention: 5-55               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-56            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-57                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-20                        --\n",
       "|    |    |    |    └─BertAttention: 5-58               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-59            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-60                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-13                       --\n",
       "|    |    └─ModuleList: 3-21                            --\n",
       "|    |    |    └─BertLayer: 4-21                        --\n",
       "|    |    |    |    └─BertAttention: 5-61               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-62            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-63                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-22                        --\n",
       "|    |    |    |    └─BertAttention: 5-64               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-65            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-66                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-14                       --\n",
       "|    |    └─ModuleList: 3-22                            --\n",
       "|    |    |    └─BertLayer: 4-23                        --\n",
       "|    |    |    |    └─BertAttention: 5-67               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-68            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-69                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-24                        --\n",
       "|    |    |    |    └─BertAttention: 5-70               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-71            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-72                  2,361,600\n",
       "├─ModuleList: 1-3                                       --\n",
       "|    └─BertPooler: 2-15                                 --\n",
       "|    |    └─Linear: 3-23                                590,592\n",
       "|    |    └─Tanh: 3-24                                  --\n",
       "|    └─BertPooler: 2-16                                 --\n",
       "|    |    └─Linear: 3-25                                590,592\n",
       "|    |    └─Tanh: 3-26                                  --\n",
       "├─ModuleList: 1-4                                       --\n",
       "|    └─Dropout: 2-17                                    --\n",
       "|    └─Dropout: 2-18                                    --\n",
       "├─ModuleList: 1-5                                       --\n",
       "|    └─Linear: 2-19                                     14,611\n",
       "|    └─Linear: 2-20                                     21,532\n",
       "├─ModuleList: 1-6                                       --\n",
       "|    └─Mixup: 2-21                                      --\n",
       "|    └─Mixup: 2-22                                      --\n",
       "================================================================================\n",
       "Total params: 217,882,415\n",
       "Trainable params: 217,882,415\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(combined_model,input_size=(768,),depth=5,batch_dim=2, dtypes=[torch.IntTensor]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a31be",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b715e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "def get_dataloader(texts,labels,idx,batch_size,shuffle):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(text,return_attention_mask=True,add_special_tokens=True,max_length = max_length,\n",
    "    padding=\"max_length\",truncation=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    lbs = torch.tensor(labels.to_numpy(),dtype=torch.float32)\n",
    "    task_idx = torch.zeros(lbs.size()).fill_(idx)\n",
    "    dataset = TensorDataset(input_ids,attention_masks,lbs,task_idx)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22401af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_train_dataloader = get_dataloader(train_df.text,train_df.iloc[:,1:],0,batch_size,shuffle=True)\n",
    "aux_train_dataloader = get_dataloader(auxiliary_train.text,emotion_labels_train,1,batch_size,shuffle=True)\n",
    "train_dataloaders = [prim_train_dataloader,aux_train_dataloader]\n",
    "\n",
    "prim_test_dataloader = get_dataloader(test_df.text,test_df.iloc[:,1:],0,batch_size,shuffle=True)\n",
    "aux_test_dataloader = get_dataloader(auxiliary_val.text,emotion_labels_val,1,batch_size,shuffle=True)\n",
    "test_dataloaders = [prim_test_dataloader,aux_test_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0df6d",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea15809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(p,targets,gamma=3,alpha=0.8):\n",
    "    bce = torch.nn.functional.binary_cross_entropy(p,targets,reduction='none')\n",
    "    p = torch.where(targets == 1,p,1-p)\n",
    "    alpha = targets * alpha + (1-targets) * (1 - alpha)\n",
    "    loss = (bce * alpha * (1 - p) ** gamma)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ac49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_combination(p_loss,a_loss,alpha=0.5):\n",
    "    return p_loss * alpha + a_loss * (1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ff06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion_params_a,criterion_params_b,lamb,criterion):\n",
    "    return criterion(*criterion_params_a) * lamb + criterion(*criterion_params_b) * (1-lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1ecc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = combined_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "  Batch   100  of  2,714.    Elapsed: 0:00:54.\n",
      "  Batch   200  of  2,714.    Elapsed: 0:01:48.\n",
      "  Batch   300  of  2,714.    Elapsed: 0:02:42.\n",
      "  Batch   400  of  2,714.    Elapsed: 0:03:36.\n",
      "  Batch   500  of  2,714.    Elapsed: 0:04:30.\n",
      "  Batch   600  of  2,714.    Elapsed: 0:05:25.\n",
      "  Batch   700  of  2,714.    Elapsed: 0:06:19.\n",
      "  Batch   800  of  2,714.    Elapsed: 0:07:14.\n",
      "  Batch   900  of  2,714.    Elapsed: 0:08:09.\n",
      "  Batch 1,000  of  2,714.    Elapsed: 0:09:03.\n",
      "  Batch 1,100  of  2,714.    Elapsed: 0:09:58.\n",
      "  Batch 1,200  of  2,714.    Elapsed: 0:10:52.\n",
      "  Batch 1,300  of  2,714.    Elapsed: 0:11:47.\n",
      "  Batch 1,400  of  2,714.    Elapsed: 0:12:42.\n",
      "  Batch 1,500  of  2,714.    Elapsed: 0:13:36.\n",
      "  Batch 1,600  of  2,714.    Elapsed: 0:14:31.\n",
      "  Batch 1,700  of  2,714.    Elapsed: 0:15:26.\n",
      "  Batch 1,800  of  2,714.    Elapsed: 0:16:21.\n",
      "  Batch 1,900  of  2,714.    Elapsed: 0:17:15.\n",
      "  Batch 2,000  of  2,714.    Elapsed: 0:18:10.\n",
      "  Batch 2,100  of  2,714.    Elapsed: 0:19:05.\n",
      "  Batch 2,200  of  2,714.    Elapsed: 0:19:59.\n",
      "  Batch 2,300  of  2,714.    Elapsed: 0:20:54.\n",
      "  Batch 2,400  of  2,714.    Elapsed: 0:21:49.\n",
      "  Batch 2,500  of  2,714.    Elapsed: 0:22:44.\n"
     ]
    }
   ],
   "source": [
    "model_vars = [i[1] for i in combined_model.named_parameters() if i[0].find(\"alpha\") == -1]\n",
    "alpha_vars = [i[1] for i in combined_model.named_parameters() if i[0].find(\"alpha\") != -1]\n",
    "\n",
    "\n",
    "prim_optimizer = torch.optim.AdamW([{\"params\":model_vars},{\"params\":alpha_vars,\"lr\":0.01}],lr=learning_rate)\n",
    "sigmoid = torch.sigmoid\n",
    "aux_optimizer = torch.optim.AdamW([{\"params\":model_vars},{\"params\":alpha_vars,\"lr\":0.01}],lr=learning_rate)\n",
    "\n",
    "\n",
    "if apply_scheduler:\n",
    "    num_train_steps = int(len(aux_train_dataloader) * num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "    prim_scheduler = get_constant_schedule_with_warmup(prim_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "    \n",
    "    aux_scheduler = get_constant_schedule_with_warmup(aux_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "    \n",
    "\n",
    "sigmoid = torch.sigmoid\n",
    "\n",
    "\n",
    "    \n",
    "loss_func = focal_loss\n",
    "multitask_loss = naive_combination\n",
    "\n",
    "best_loss_prim = np.inf\n",
    "\n",
    "best_auc_prim = 0\n",
    "\n",
    "best_loss_aux = np.inf\n",
    "\n",
    "best_auc_aux = 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss_batch(batch,model,criterion):\n",
    "    \n",
    "    input_ids = batch[0].to(device)\n",
    "    input_masks = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "    task_idx = int(batch[3][0][0].item())\n",
    "    \n",
    "    #print(batch)\n",
    "    \n",
    "    if model.training:\n",
    "        \n",
    "        loss_f = lambda x,y,z: mixup_criterion(x,y,z,criterion=criterion)\n",
    "\n",
    "        outputs,labels_a,labels_b,lamb = model(input_ids,input_masks,labels,task_idx)\n",
    "\n",
    "        return loss_f((sigmoid(outputs),labels_a),(sigmoid(outputs),labels_b),lamb)\n",
    "    else:\n",
    "        \n",
    "        outputs = model(input_ids,input_masks,labels,task_idx)\n",
    "        return criterion(sigmoid(outputs),labels), outputs,labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for epoch_i in range(0,num_train_epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tr_loss = 0\n",
    "    pr_loss = 0\n",
    "    aux_loss = 0\n",
    "    \n",
    "    combined_model.train()\n",
    "    prim_iter = iter(prim_train_dataloader)\n",
    "    aux_iter = iter(aux_train_dataloader)\n",
    "    \n",
    "    #no caso tô assumindo que o aux é sempre maior, porque esse é meu caso agora\n",
    "    \n",
    "    for step,aux_batch in enumerate(aux_iter):\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(aux_train_dataloader), elapsed))\n",
    "        try:\n",
    "            prim_batch = prim_iter.__next__()\n",
    "        except StopIteration:\n",
    "            prim_iter = iter(prim_train_dataloader)\n",
    "            prim_batch = prim_iter.__next__()\n",
    "        \n",
    "        prim_loss = calculate_loss_batch(prim_batch,combined_model,loss_func)\n",
    "        \n",
    "        prim_optimizer.zero_grad()\n",
    "        prim_loss.backward()\n",
    "        prim_optimizer.step()\n",
    "        \n",
    "        if apply_scheduler:\n",
    "            prim_scheduler.step()\n",
    "        \n",
    "        \n",
    "        aux_loss = calculate_loss_batch(aux_batch,combined_model,loss_func)\n",
    "        \n",
    "        aux_optimizer.zero_grad()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "        \n",
    "        if apply_scheduler:\n",
    "            aux_scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_loss = multitask_loss(prim_loss,aux_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "            tr_loss += batch_loss.item()\n",
    "        \n",
    "            pr_loss += prim_loss.item()\n",
    "        \n",
    "            aux_loss += aux_loss.item()\n",
    "\n",
    "        \n",
    "    \n",
    "    avg_prim_loss = pr_loss/len(aux_train_dataloader)    \n",
    "    avg_train_loss = tr_loss/len(aux_train_dataloader)\n",
    "    avg_aux_loss = aux_loss/len(aux_train_dataloader)    \n",
    "\n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {:}\".format(avg_train_loss))\n",
    "    print(\"  Average primary loss: {:}\".format(avg_prim_loss))\n",
    "    print(\"  Average aux loss: {:}\".format(avg_aux_loss))\n",
    "\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    \n",
    "    print(\"  Testing primary...\")\n",
    "    combined_model.eval()\n",
    "    \n",
    "    pr_loss = 0\n",
    "    tt_loss = 0\n",
    "    aux_loss = 0\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels_ids = []\n",
    "    \n",
    "    for prim_batch in prim_test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            prim_loss,outputs,labels = calculate_loss_batch(prim_batch,combined_model,loss_func)\n",
    "            probs = sigmoid(outputs)\n",
    "            pr_loss += prim_loss.item()\n",
    "            \n",
    "        all_probs += probs.detach().cpu()\n",
    "        all_labels_ids += labels.detach().cpu()\n",
    "        \n",
    "    all_probs = torch.stack(all_probs).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    \n",
    "    cur_auc = average_precision_score(all_labels_ids,all_probs)\n",
    "\n",
    "    \n",
    "    avg_prim_test_loss = pr_loss/len(prim_test_dataloader)\n",
    "    \n",
    "    if cur_auc > best_auc_prim:\n",
    "        best_auc_prim = cur_auc\n",
    "        \n",
    "    if avg_prim_test_loss < best_loss_prim:\n",
    "        best_loss = avg_prim_test_loss\n",
    "        best_probs_prim = all_probs\n",
    "        best_labels_prim = all_labels_ids\n",
    "        \n",
    "    print(\"  AUC primária:\",cur_auc)\n",
    "    print(\"  Average test primary loss: {:}\".format(avg_prim_test_loss))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"  Testing auxiliary...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    aux_loss = 0\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels_ids = []\n",
    "    \n",
    "    for prim_batch in aux_test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            prim_loss,outputs,labels = calculate_loss_batch(prim_batch,combined_model,loss_func)\n",
    "            probs = sigmoid(outputs)\n",
    "            aux_loss += prim_loss.item()\n",
    "            \n",
    "        all_probs += probs.detach().cpu()\n",
    "        all_labels_ids += labels.detach().cpu()\n",
    "        \n",
    "    all_probs = torch.stack(all_probs).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    \n",
    "    cur_auc = average_precision_score(all_labels_ids,all_probs)\n",
    "\n",
    "    \n",
    "    avg_aux_test_loss = aux_loss/len(prim_test_dataloader)  \n",
    "    \n",
    "    if cur_auc > best_auc_aux:\n",
    "        best_auc_aux = cur_auc\n",
    "        \n",
    "    if avg_aux_test_loss < best_loss_aux:\n",
    "        best_loss = avg_aux_test_loss\n",
    "        best_probs_aux = all_probs\n",
    "        best_labels_aux = all_labels_ids\n",
    "        \n",
    "    print(\"  AUC auxiliar:\",cur_auc)\n",
    "    avg_test_loss = multitask_loss(avg_prim_test_loss,avg_aux_test_loss)\n",
    "    \n",
    "    print(\"  Average test loss: {:}\".format(avg_test_loss))\n",
    "    print(\"  Average test aux loss: {:}\".format(avg_aux_test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328fbfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sharing_layer in combined_model.soft_sharing_layers:\n",
    "    print(sharing_layer.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1eded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
