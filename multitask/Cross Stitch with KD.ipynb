{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24775f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pra conseguir algum erro util da porra do torch\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0666d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, get_constant_schedule_with_warmup, BertForSequenceClassification\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.skmultilearn.model_selection.iterative_stratification import iterative_train_test_split, IterativeStratification\n",
    "from sklearn.metrics import classification_report, average_precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71705c4",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b5fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4, min_delta=0,is_loss=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.is_loss = is_loss\n",
    "        if is_loss:\n",
    "            self.min_validation_loss = np.inf\n",
    "        else:\n",
    "            self.min_validation_loss = -np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if self.is_loss:\n",
    "            if validation_loss < self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW LOWEST LOSS \",self.min_validation_loss)\n",
    "            elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            if validation_loss > self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW HIGHEST SCORE \",self.min_validation_loss)\n",
    "            elif validation_loss <= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86eb69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f73e5",
   "metadata": {},
   "source": [
    "# Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276a9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 2023\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "  device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a560d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_train_epochs = 20\n",
    "print_each_n_step = 100\n",
    "batch_size= 16\n",
    "warmup_proportion = 0.2\n",
    "apply_scheduler = True\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5249173",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff7194",
   "metadata": {},
   "source": [
    "## Tarefa primária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1017af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    df['Alteração na eficiência/funcionalidade'] = (df['Alteração na eficiência/funcionalidade'] == 1) | (df[\"Alteração da funcionalidade\"] == 1) | (df[\"Alteração na eficiência\"] == 1)\n",
    "    df = df.drop([\"Postagem com possível perfil depressivo\",\"Alteração na eficiência\",\n",
    "         \"Alteração da funcionalidade\",\"*\",'Agitação/inquietação','Sintoma obsessivo e compulsivo','Déficit de atenção/Memória',\n",
    "              'Perda/Diminuição do prazer/ Perda/Diminuição da libido'],axis=1)\n",
    "    df['Neutro'] =  df.iloc[:,1:].sum(axis=1) == 0\n",
    "    df = df.replace({True:1,False:0})\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9cea857",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_data(pd.read_csv(\"data/segredos_sentenças_multitask_train_clean.csv\",index_col=0))\n",
    "test_df = process_data(pd.read_csv(\"data/segredos_sentenças_multitask_test_clean.csv\",index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3650e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symptom_num = train_df.iloc[:,1:].shape[1]\n",
    "target_names_primary = list(train_df.iloc[:,1:].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c129b",
   "metadata": {},
   "source": [
    "## Tarefa auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c02ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_emotions_path = \"data/goemotions\"\n",
    "\n",
    "auxiliary_train = pd.read_csv(f\"{go_emotions_path}/train.tsv\",sep='\\t')\n",
    "auxiliary_val = pd.read_csv(f\"{go_emotions_path}/dev.tsv\",sep='\\t')\n",
    "\n",
    "auxiliary_test = pd.read_csv(f\"{go_emotions_path}/test.tsv\",sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23012bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label_encoding(row):\n",
    "    #mudar encoding para ser multirrótulo (preferível para o cálculo da entropia)\n",
    "    labels = row['labels'].replace(\" \",\"\").split(\",\")\n",
    "    new_row = {}\n",
    "    for emotion in emotion_dict:\n",
    "        if str(emotion_dict[emotion]) in labels:\n",
    "            new_row[emotion] = True\n",
    "        else:\n",
    "            new_row[emotion] = False\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c6eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {\n",
    "    \"admiração\": 0,\n",
    "    \"diversão\": 1,\n",
    "    \"raiva\": 2,\n",
    "    \"aborrecimento\": 3,\n",
    "    \"aprovação\": 4,\n",
    "    \"zelo\": 5,\n",
    "    \"confusão\": 6,\n",
    "    \"curiosidade\": 7,\n",
    "    \"desejo\": 8,\n",
    "    \"decepção\": 9,\n",
    "    \"desaprovação\": 10,\n",
    "    \"nojo\": 11,\n",
    "    \"constrangimento\": 12,\n",
    "    \"entusiasmo\": 13,\n",
    "    \"medo\": 14,\n",
    "    \"gratidão\": 15,\n",
    "    \"luto\": 16,\n",
    "    \"alegria\": 17,\n",
    "    \"amor\": 18,\n",
    "    \"nervosismo\": 19,\n",
    "    \"otimismo\": 20,\n",
    "    \"orgulho\": 21,\n",
    "    \"percepção\": 22,\n",
    "    \"alívio\": 23,\n",
    "    \"remorso\": 24,\n",
    "    \"tristeza\": 25,\n",
    "    \"surpresa\": 26,\n",
    "    \"neutro\": 27\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd40c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels_test = pd.DataFrame(list(auxiliary_test.apply(change_label_encoding,axis=1)))\n",
    "emotion_labels_train = pd.DataFrame(list(auxiliary_train.apply(change_label_encoding,axis=1)))\n",
    "emotion_labels_val = pd.DataFrame(list(auxiliary_val.apply(change_label_encoding,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "358db423",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_num = len(emotion_dict)\n",
    "target_names_auxiliary = list(auxiliary_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2caca012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiração</th>\n",
       "      <th>diversão</th>\n",
       "      <th>raiva</th>\n",
       "      <th>aborrecimento</th>\n",
       "      <th>aprovação</th>\n",
       "      <th>zelo</th>\n",
       "      <th>confusão</th>\n",
       "      <th>curiosidade</th>\n",
       "      <th>desejo</th>\n",
       "      <th>decepção</th>\n",
       "      <th>...</th>\n",
       "      <th>amor</th>\n",
       "      <th>nervosismo</th>\n",
       "      <th>otimismo</th>\n",
       "      <th>orgulho</th>\n",
       "      <th>percepção</th>\n",
       "      <th>alívio</th>\n",
       "      <th>remorso</th>\n",
       "      <th>tristeza</th>\n",
       "      <th>surpresa</th>\n",
       "      <th>neutro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       admiração  diversão  raiva  aborrecimento  aprovação   zelo  confusão  \\\n",
       "0          False     False  False          False      False  False     False   \n",
       "1          False     False  False          False      False  False     False   \n",
       "2          False     False   True          False      False  False     False   \n",
       "3          False     False  False          False      False  False     False   \n",
       "4          False     False  False           True      False  False     False   \n",
       "...          ...       ...    ...            ...        ...    ...       ...   \n",
       "43405      False     False  False          False      False  False     False   \n",
       "43406      False     False  False          False      False  False      True   \n",
       "43407      False     False  False           True      False  False     False   \n",
       "43408      False     False  False          False      False  False     False   \n",
       "43409      False     False  False          False      False  False     False   \n",
       "\n",
       "       curiosidade  desejo  decepção  ...   amor  nervosismo  otimismo  \\\n",
       "0            False   False     False  ...  False       False     False   \n",
       "1            False   False     False  ...  False       False     False   \n",
       "2            False   False     False  ...  False       False     False   \n",
       "3            False   False     False  ...  False       False     False   \n",
       "4            False   False     False  ...  False       False     False   \n",
       "...            ...     ...       ...  ...    ...         ...       ...   \n",
       "43405        False   False     False  ...   True       False     False   \n",
       "43406        False   False     False  ...  False       False     False   \n",
       "43407        False   False     False  ...  False       False     False   \n",
       "43408        False   False     False  ...  False       False     False   \n",
       "43409        False   False     False  ...  False       False     False   \n",
       "\n",
       "       orgulho  percepção  alívio  remorso  tristeza  surpresa  neutro  \n",
       "0        False      False   False    False     False     False    True  \n",
       "1        False      False   False    False     False     False    True  \n",
       "2        False      False   False    False     False     False   False  \n",
       "3        False      False   False    False     False     False   False  \n",
       "4        False      False   False    False     False     False   False  \n",
       "...        ...        ...     ...      ...       ...       ...     ...  \n",
       "43405    False      False   False    False     False     False   False  \n",
       "43406    False      False   False    False     False     False   False  \n",
       "43407    False      False   False    False     False     False   False  \n",
       "43408    False      False   False    False     False     False   False  \n",
       "43409    False      False   False    False     False     False   False  \n",
       "\n",
       "[43410 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d1bd1",
   "metadata": {},
   "source": [
    "# Conjunto de logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ef7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_df = pd.read_csv(\"features/both tasks facebook + reddit + goemotions.csv\",index_col = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "839ec11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_reddit = logits_df.loc[train_df.shape[0]: logits_df.shape[0] - emotion_labels_train.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c81df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "#pegando só uma parte dos dados do goemotions. Não é necessário treinar no conjunto completo\n",
    "strat = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=seed_val).split(auxiliary_train,emotion_labels_train)\n",
    "\n",
    "for train_idx, test_idx in strat:\n",
    "    \n",
    "    logits_emotions = logits_df.tail(emotion_labels_train.shape[0]).reset_index(drop=True)\n",
    "    emotion_labels_train = emotion_labels_train.loc[test_idx]\n",
    "    auxiliary_train = auxiliary_train.loc[test_idx]\n",
    "    logits_emotions = logits_df.loc[test_idx]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba491b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_symptoms = logits_df.head(train_df.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a14f21",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d05452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(torch.nn.Module):\n",
    "    def __init__(self,mixup_alpha=1):\n",
    "        \n",
    "        super(Mixup,self).__init__()\n",
    "    \n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "        \n",
    "    def mixup(self,batch_ids,batch_labels,alpha=1):\n",
    "        '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = batch_ids.size()[0]\n",
    "\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "\n",
    "        mixed_x = lam * batch_ids + (1 - lam) * batch_ids[index, :]\n",
    "        #using bigger input mask\n",
    "        #mixed_masks = batch_masks | batch_masks[index,:]\n",
    "        y_a, y_b = batch_labels, batch_labels[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "    def forward(self,outputs,labels):\n",
    "       \n",
    "        if self.training:\n",
    "            outputs, labels_a, labels_b, lamb = self.mixup(outputs,labels,self.mixup_alpha)\n",
    "\n",
    "            return outputs,labels_a,labels_b,lamb\n",
    "        else:\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2988c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSoftSharingLayer(nn.Module):\n",
    "    def __init__(self,encoders):\n",
    "        super(BertSoftSharingLayer,self).__init__()\n",
    "\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.n_tasks = len(encoders)\n",
    "        #linear combination weights between tasks\n",
    "        #self.alpha = nn.Parameter(torch.eye(len(encoders),len(encoders),requires_grad=True))\n",
    "        #self.alpha = nn.Parameter(torch.full((len(encoders),len(encoders)),0.5,requires_grad=True))\n",
    "        \n",
    "        self.alpha_prim = nn.Parameter(torch.tensor(0.5,requires_grad=True))\n",
    "        self.beta_prim = nn.Parameter(torch.tensor(0.5,requires_grad=True))\n",
    "        \n",
    "        self.alpha_aux = nn.Parameter(torch.tensor(0.5,requires_grad=True))\n",
    "        self.beta_aux = nn.Parameter(torch.tensor(0.5,requires_grad=True))\n",
    "        \n",
    "        self.alphas = [self.alpha_prim,self.alpha_aux]\n",
    "        self.betas = [self.beta_prim,self.beta_aux]\n",
    "\n",
    "    def prepare_for_task(self,task_idx):\n",
    "        #notar que os alphas não foram tocados. Estes são sempre treinados\n",
    "        for n in range(self.n_tasks):\n",
    "            if n != task_idx:\n",
    "                for param in self.encoders[n].parameters():\n",
    "                    param.requires_grad=False\n",
    "                    self.alphas[n].requires_grad=False\n",
    "                    self.betas[n].requires_grad=False\n",
    "            else:\n",
    "                for param in self.encoders[n].parameters():\n",
    "                    param.requires_grad=True\n",
    "                    self.alphas[n].requires_grad=True\n",
    "                    self.betas[n].requires_grad=True\n",
    "\n",
    "        \n",
    "    def forward(self,input_arr,attention_mask):\n",
    "        outputs = []\n",
    "        for i,encoder in enumerate(self.encoders):\n",
    "            outputs.append(encoder(input_arr[i],attention_mask = attention_mask)[0])\n",
    "        \n",
    "        outputs[0] = outputs[0] * self.alpha_prim + outputs[1] * self.beta_prim\n",
    "        outputs[1] = outputs[1] * self.alpha_aux + outputs[0] * self.beta_aux\n",
    "\n",
    "        outputs = torch.stack(outputs)\n",
    "\n",
    "        #linear_combination = torch.matmul(self.alpha,outputs.reshape(self.n_tasks,torch.prod(torch.tensor(outputs.size()[1:]))))\n",
    "        #return linear_combination.reshape(outputs.size())\n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e93b3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSharedParametersModel(nn.Module):\n",
    "    def __init__(self,models,attention_n,hidden_size,num_labels,dropout_rate=0.3,mixup_alphas=[1,1],mixup_layer=12):\n",
    "        super(BertSharedParametersModel,self).__init__()\n",
    "        self.n_tasks = len(models)\n",
    "        self.embeddings = nn.ModuleList([model.bert.embeddings for model in models])\n",
    "        soft_sharing_layers = []\n",
    "        #assumindo que os modelos têm o mesmo n° de camadas de atenção\n",
    "        for i in range(attention_n):\n",
    "            attention_layers = [model.bert.encoder.layer[i] for model in models]\n",
    "            soft_sharing_layers.append(BertSoftSharingLayer(attention_layers))\n",
    "        self.soft_sharing_layers = nn.ModuleList(soft_sharing_layers)\n",
    "        self.poolers = nn.ModuleList([model.bert.pooler for model in models])\n",
    "        self.dropouts = nn.ModuleList([model.dropout for model in models])\n",
    "        self.classification_heads = nn.ModuleList([model.classifier for model in models])\n",
    "        self.mixups = nn.ModuleList([Mixup(alpha) for alpha in mixup_alphas])\n",
    "        self.mixup_layer = mixup_layer\n",
    "        for i,_ in enumerate(self.dropouts):\n",
    "            self.dropouts.p = dropout_rate\n",
    "            \n",
    "            \n",
    "    def prepare_for_task(self,task_idx):\n",
    "        for n in range(self.n_tasks):\n",
    "            if n == task_idx:\n",
    "                for param in self.embeddings[n].parameters():\n",
    "                    param.requires_grad= True\n",
    "                \n",
    "                for i in range (len(self.soft_sharing_layers)):\n",
    "                    self.soft_sharing_layers[i].prepare_for_task(task_idx)\n",
    "                \n",
    "                for param in self.poolers[n].parameters():\n",
    "                    param.requires_grad= True\n",
    "                \n",
    "                for param in self.classification_heads[n].parameters():\n",
    "                    param_requires_grad = True\n",
    "            else:\n",
    "                \n",
    "                for param in self.embeddings[n].parameters():\n",
    "                    param.requires_grad= False\n",
    "                \n",
    "                for param in self.poolers[n].parameters():\n",
    "                    param.requires_grad= False\n",
    "                \n",
    "                for param in self.classification_heads[n].parameters():\n",
    "                    param_requires_grad = False\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self,input_arr,attention_mask,labels,task_idx):\n",
    "        \n",
    "        self.prepare_for_task(task_idx)\n",
    "\n",
    "        \n",
    "        \n",
    "        extended_attention_mask: extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float)  \n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(torch.float).min\n",
    "        outputs = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "           \n",
    "            outputs.append(embedding(input_arr))\n",
    "            \n",
    "        outputs = torch.stack(outputs)\n",
    "        \n",
    "\n",
    "        for i,sharing_layer in enumerate(self.soft_sharing_layers):\n",
    "            outputs = sharing_layer(outputs,attention_mask=extended_attention_mask)\n",
    "            if i == (self.mixup_layer-1) and self.training:\n",
    "                outputs_cur_task,labels_a,labels_b,lamb = self.mixups[task_idx](outputs[task_idx],labels)\n",
    "                outputs[task_idx] = outputs_cur_task\n",
    "        outputs = self.poolers[task_idx](outputs[task_idx])\n",
    "        \n",
    "        \n",
    "        outputs = self.dropouts[task_idx](outputs)\n",
    "            \n",
    "\n",
    "        outputs = self.classification_heads[task_idx](outputs)\n",
    "        if self.training:\n",
    "            return outputs,labels_a,labels_b,lamb\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a558e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_model.prepare_for_task(5)\n",
    "#for param in combined_model.soft_sharing_layers[2].encoders[0].parameters():\n",
    "#    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f15c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#primary_model_path = \"models/BERT baselines\"\n",
    "primary_model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "#auxiliary_model_path = \"models/go_emotions\"\n",
    "auxiliary_model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "\n",
    "tokenizer_path = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bacde06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "primary_model = BertForSequenceClassification.from_pretrained(primary_model_path,num_labels=symptom_num)\n",
    "auxiliary_model = BertForSequenceClassification.from_pretrained(auxiliary_model_path,num_labels=emotion_num)\n",
    "\n",
    "primary_config = AutoConfig.from_pretrained(primary_model_path)\n",
    "auxiliary_config = AutoConfig.from_pretrained(auxiliary_model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40d17069",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert primary_config.num_hidden_layers == auxiliary_config.num_hidden_layers\n",
    "combined_model = BertSharedParametersModel([primary_model,auxiliary_model],primary_config.num_hidden_layers,\n",
    "                                          primary_config.hidden_size,[symptom_num,emotion_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc339abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "├─ModuleList: 1-1                                       --\n",
      "|    └─BertEmbeddings: 2-1                              --\n",
      "|    |    └─Embedding: 3-1                              22,881,792\n",
      "|    |    └─Embedding: 3-2                              393,216\n",
      "|    |    └─Embedding: 3-3                              1,536\n",
      "|    |    └─LayerNorm: 3-4                              1,536\n",
      "|    |    └─Dropout: 3-5                                --\n",
      "|    └─BertEmbeddings: 2-2                              --\n",
      "|    |    └─Embedding: 3-6                              22,881,792\n",
      "|    |    └─Embedding: 3-7                              393,216\n",
      "|    |    └─Embedding: 3-8                              1,536\n",
      "|    |    └─LayerNorm: 3-9                              1,536\n",
      "|    |    └─Dropout: 3-10                               --\n",
      "├─ModuleList: 1-2                                       --\n",
      "|    └─BertSoftSharingLayer: 2-3                        --\n",
      "|    |    └─ModuleList: 3-11                            --\n",
      "|    |    |    └─BertLayer: 4-1                         --\n",
      "|    |    |    |    └─BertAttention: 5-1                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-2             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-3                   2,361,600\n",
      "|    |    |    └─BertLayer: 4-2                         --\n",
      "|    |    |    |    └─BertAttention: 5-4                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-5             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-6                   2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-4                        --\n",
      "|    |    └─ModuleList: 3-12                            --\n",
      "|    |    |    └─BertLayer: 4-3                         --\n",
      "|    |    |    |    └─BertAttention: 5-7                2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-8             2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-9                   2,361,600\n",
      "|    |    |    └─BertLayer: 4-4                         --\n",
      "|    |    |    |    └─BertAttention: 5-10               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-11            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-12                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-5                        --\n",
      "|    |    └─ModuleList: 3-13                            --\n",
      "|    |    |    └─BertLayer: 4-5                         --\n",
      "|    |    |    |    └─BertAttention: 5-13               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-14            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-15                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-6                         --\n",
      "|    |    |    |    └─BertAttention: 5-16               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-17            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-18                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-6                        --\n",
      "|    |    └─ModuleList: 3-14                            --\n",
      "|    |    |    └─BertLayer: 4-7                         --\n",
      "|    |    |    |    └─BertAttention: 5-19               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-20            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-21                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-8                         --\n",
      "|    |    |    |    └─BertAttention: 5-22               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-23            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-24                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-7                        --\n",
      "|    |    └─ModuleList: 3-15                            --\n",
      "|    |    |    └─BertLayer: 4-9                         --\n",
      "|    |    |    |    └─BertAttention: 5-25               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-26            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-27                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-10                        --\n",
      "|    |    |    |    └─BertAttention: 5-28               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-29            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-30                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-8                        --\n",
      "|    |    └─ModuleList: 3-16                            --\n",
      "|    |    |    └─BertLayer: 4-11                        --\n",
      "|    |    |    |    └─BertAttention: 5-31               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-32            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-33                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-12                        --\n",
      "|    |    |    |    └─BertAttention: 5-34               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-35            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-36                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-9                        --\n",
      "|    |    └─ModuleList: 3-17                            --\n",
      "|    |    |    └─BertLayer: 4-13                        --\n",
      "|    |    |    |    └─BertAttention: 5-37               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-38            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-39                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-14                        --\n",
      "|    |    |    |    └─BertAttention: 5-40               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-41            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-42                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-10                       --\n",
      "|    |    └─ModuleList: 3-18                            --\n",
      "|    |    |    └─BertLayer: 4-15                        --\n",
      "|    |    |    |    └─BertAttention: 5-43               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-44            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-45                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-16                        --\n",
      "|    |    |    |    └─BertAttention: 5-46               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-47            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-48                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-11                       --\n",
      "|    |    └─ModuleList: 3-19                            --\n",
      "|    |    |    └─BertLayer: 4-17                        --\n",
      "|    |    |    |    └─BertAttention: 5-49               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-50            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-51                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-18                        --\n",
      "|    |    |    |    └─BertAttention: 5-52               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-53            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-54                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-12                       --\n",
      "|    |    └─ModuleList: 3-20                            --\n",
      "|    |    |    └─BertLayer: 4-19                        --\n",
      "|    |    |    |    └─BertAttention: 5-55               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-56            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-57                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-20                        --\n",
      "|    |    |    |    └─BertAttention: 5-58               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-59            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-60                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-13                       --\n",
      "|    |    └─ModuleList: 3-21                            --\n",
      "|    |    |    └─BertLayer: 4-21                        --\n",
      "|    |    |    |    └─BertAttention: 5-61               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-62            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-63                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-22                        --\n",
      "|    |    |    |    └─BertAttention: 5-64               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-65            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-66                  2,361,600\n",
      "|    └─BertSoftSharingLayer: 2-14                       --\n",
      "|    |    └─ModuleList: 3-22                            --\n",
      "|    |    |    └─BertLayer: 4-23                        --\n",
      "|    |    |    |    └─BertAttention: 5-67               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-68            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-69                  2,361,600\n",
      "|    |    |    └─BertLayer: 4-24                        --\n",
      "|    |    |    |    └─BertAttention: 5-70               2,363,904\n",
      "|    |    |    |    └─BertIntermediate: 5-71            2,362,368\n",
      "|    |    |    |    └─BertOutput: 5-72                  2,361,600\n",
      "├─ModuleList: 1-3                                       --\n",
      "|    └─BertPooler: 2-15                                 --\n",
      "|    |    └─Linear: 3-23                                590,592\n",
      "|    |    └─Tanh: 3-24                                  --\n",
      "|    └─BertPooler: 2-16                                 --\n",
      "|    |    └─Linear: 3-25                                590,592\n",
      "|    |    └─Tanh: 3-26                                  --\n",
      "├─ModuleList: 1-4                                       --\n",
      "|    └─Dropout: 2-17                                    --\n",
      "|    └─Dropout: 2-18                                    --\n",
      "├─ModuleList: 1-5                                       --\n",
      "|    └─Linear: 2-19                                     14,611\n",
      "|    └─Linear: 2-20                                     21,532\n",
      "├─ModuleList: 1-6                                       --\n",
      "|    └─Mixup: 2-21                                      --\n",
      "|    └─Mixup: 2-22                                      --\n",
      "================================================================================\n",
      "Total params: 217,882,415\n",
      "Trainable params: 217,882,415\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "├─ModuleList: 1-1                                       --\n",
       "|    └─BertEmbeddings: 2-1                              --\n",
       "|    |    └─Embedding: 3-1                              22,881,792\n",
       "|    |    └─Embedding: 3-2                              393,216\n",
       "|    |    └─Embedding: 3-3                              1,536\n",
       "|    |    └─LayerNorm: 3-4                              1,536\n",
       "|    |    └─Dropout: 3-5                                --\n",
       "|    └─BertEmbeddings: 2-2                              --\n",
       "|    |    └─Embedding: 3-6                              22,881,792\n",
       "|    |    └─Embedding: 3-7                              393,216\n",
       "|    |    └─Embedding: 3-8                              1,536\n",
       "|    |    └─LayerNorm: 3-9                              1,536\n",
       "|    |    └─Dropout: 3-10                               --\n",
       "├─ModuleList: 1-2                                       --\n",
       "|    └─BertSoftSharingLayer: 2-3                        --\n",
       "|    |    └─ModuleList: 3-11                            --\n",
       "|    |    |    └─BertLayer: 4-1                         --\n",
       "|    |    |    |    └─BertAttention: 5-1                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-2             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-3                   2,361,600\n",
       "|    |    |    └─BertLayer: 4-2                         --\n",
       "|    |    |    |    └─BertAttention: 5-4                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-5             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-6                   2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-4                        --\n",
       "|    |    └─ModuleList: 3-12                            --\n",
       "|    |    |    └─BertLayer: 4-3                         --\n",
       "|    |    |    |    └─BertAttention: 5-7                2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-8             2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-9                   2,361,600\n",
       "|    |    |    └─BertLayer: 4-4                         --\n",
       "|    |    |    |    └─BertAttention: 5-10               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-11            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-12                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-5                        --\n",
       "|    |    └─ModuleList: 3-13                            --\n",
       "|    |    |    └─BertLayer: 4-5                         --\n",
       "|    |    |    |    └─BertAttention: 5-13               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-14            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-15                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-6                         --\n",
       "|    |    |    |    └─BertAttention: 5-16               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-17            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-18                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-6                        --\n",
       "|    |    └─ModuleList: 3-14                            --\n",
       "|    |    |    └─BertLayer: 4-7                         --\n",
       "|    |    |    |    └─BertAttention: 5-19               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-20            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-21                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-8                         --\n",
       "|    |    |    |    └─BertAttention: 5-22               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-23            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-24                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-7                        --\n",
       "|    |    └─ModuleList: 3-15                            --\n",
       "|    |    |    └─BertLayer: 4-9                         --\n",
       "|    |    |    |    └─BertAttention: 5-25               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-26            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-27                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-10                        --\n",
       "|    |    |    |    └─BertAttention: 5-28               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-29            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-30                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-8                        --\n",
       "|    |    └─ModuleList: 3-16                            --\n",
       "|    |    |    └─BertLayer: 4-11                        --\n",
       "|    |    |    |    └─BertAttention: 5-31               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-32            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-33                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-12                        --\n",
       "|    |    |    |    └─BertAttention: 5-34               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-35            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-36                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-9                        --\n",
       "|    |    └─ModuleList: 3-17                            --\n",
       "|    |    |    └─BertLayer: 4-13                        --\n",
       "|    |    |    |    └─BertAttention: 5-37               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-38            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-39                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-14                        --\n",
       "|    |    |    |    └─BertAttention: 5-40               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-41            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-42                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-10                       --\n",
       "|    |    └─ModuleList: 3-18                            --\n",
       "|    |    |    └─BertLayer: 4-15                        --\n",
       "|    |    |    |    └─BertAttention: 5-43               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-44            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-45                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-16                        --\n",
       "|    |    |    |    └─BertAttention: 5-46               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-47            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-48                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-11                       --\n",
       "|    |    └─ModuleList: 3-19                            --\n",
       "|    |    |    └─BertLayer: 4-17                        --\n",
       "|    |    |    |    └─BertAttention: 5-49               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-50            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-51                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-18                        --\n",
       "|    |    |    |    └─BertAttention: 5-52               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-53            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-54                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-12                       --\n",
       "|    |    └─ModuleList: 3-20                            --\n",
       "|    |    |    └─BertLayer: 4-19                        --\n",
       "|    |    |    |    └─BertAttention: 5-55               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-56            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-57                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-20                        --\n",
       "|    |    |    |    └─BertAttention: 5-58               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-59            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-60                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-13                       --\n",
       "|    |    └─ModuleList: 3-21                            --\n",
       "|    |    |    └─BertLayer: 4-21                        --\n",
       "|    |    |    |    └─BertAttention: 5-61               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-62            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-63                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-22                        --\n",
       "|    |    |    |    └─BertAttention: 5-64               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-65            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-66                  2,361,600\n",
       "|    └─BertSoftSharingLayer: 2-14                       --\n",
       "|    |    └─ModuleList: 3-22                            --\n",
       "|    |    |    └─BertLayer: 4-23                        --\n",
       "|    |    |    |    └─BertAttention: 5-67               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-68            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-69                  2,361,600\n",
       "|    |    |    └─BertLayer: 4-24                        --\n",
       "|    |    |    |    └─BertAttention: 5-70               2,363,904\n",
       "|    |    |    |    └─BertIntermediate: 5-71            2,362,368\n",
       "|    |    |    |    └─BertOutput: 5-72                  2,361,600\n",
       "├─ModuleList: 1-3                                       --\n",
       "|    └─BertPooler: 2-15                                 --\n",
       "|    |    └─Linear: 3-23                                590,592\n",
       "|    |    └─Tanh: 3-24                                  --\n",
       "|    └─BertPooler: 2-16                                 --\n",
       "|    |    └─Linear: 3-25                                590,592\n",
       "|    |    └─Tanh: 3-26                                  --\n",
       "├─ModuleList: 1-4                                       --\n",
       "|    └─Dropout: 2-17                                    --\n",
       "|    └─Dropout: 2-18                                    --\n",
       "├─ModuleList: 1-5                                       --\n",
       "|    └─Linear: 2-19                                     14,611\n",
       "|    └─Linear: 2-20                                     21,532\n",
       "├─ModuleList: 1-6                                       --\n",
       "|    └─Mixup: 2-21                                      --\n",
       "|    └─Mixup: 2-22                                      --\n",
       "================================================================================\n",
       "Total params: 217,882,415\n",
       "Trainable params: 217,882,415\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(combined_model,input_size=(768,),depth=5,batch_dim=2, dtypes=[torch.IntTensor]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a31be",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b715e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "def get_dataloader(texts,labels,idx,batch_size,shuffle):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(text,return_attention_mask=True,add_special_tokens=True,max_length = max_length,\n",
    "    padding=\"max_length\",truncation=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    lbs = torch.tensor(labels.to_numpy(),dtype=torch.float32)\n",
    "    task_idx = torch.zeros(lbs.size()).fill_(idx)\n",
    "    dataset = TensorDataset(input_ids,attention_masks,lbs,task_idx)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "\n",
    "\n",
    "def fill_unlabeled(symptoms,df,symptom_num):\n",
    "    data = []\n",
    "    for row in range(df.shape[0] - symptoms.shape[0]):\n",
    "        data.append([-1] * symptom_num)\n",
    "    data = pd.DataFrame(data)\n",
    "    data.columns = symptoms.columns\n",
    "    symptoms = pd.concat([symptoms,data],axis=0)\n",
    "    return symptoms.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_logit_dataloader(texts,logits,labels,idx,batch_size,shuffle):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(text,return_attention_mask=True,add_special_tokens=True,max_length = max_length,\n",
    "    padding=\"max_length\",truncation=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    labels = fill_unlabeled(labels,logits,labels.shape[1])\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    logits = torch.tensor(logits.to_numpy(),dtype=torch.float32)\n",
    "    lbs = torch.tensor(labels.to_numpy(),dtype=torch.float32)\n",
    "    task_idx = torch.zeros(lbs.size()).fill_(idx)\n",
    "    dataset = TensorDataset(input_ids,attention_masks,logits,lbs,task_idx)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22401af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([logits_symptoms,logits_reddit])\n",
    "prim_train_dataloader = get_logit_dataloader(temp.text,temp.iloc[:,:symptom_num],train_df.iloc[:,1:],0,batch_size,shuffle=True)\n",
    "\n",
    "temp = pd.concat([logits_emotions,logits_reddit])\n",
    "aux_train_dataloader = get_logit_dataloader(temp.text,temp.iloc[:,symptom_num+1:symptom_num+emotion_num+1],emotion_labels_train,1,batch_size,shuffle=True)\n",
    "train_dataloaders = [prim_train_dataloader,aux_train_dataloader]\n",
    "\n",
    "prim_test_dataloader = get_dataloader(test_df.text,test_df.iloc[:,1:],0,batch_size,shuffle=True)\n",
    "aux_test_dataloader = get_dataloader(auxiliary_val.text,emotion_labels_val,1,batch_size,shuffle=True)\n",
    "test_dataloaders = [prim_test_dataloader,aux_test_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0df6d",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea15809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(p,targets,gamma=3,alpha=0.8):\n",
    "    bce = torch.nn.functional.binary_cross_entropy(p,targets,reduction='none')\n",
    "    p = torch.where(targets == 1,p,1-p)\n",
    "    alpha = targets * alpha + (1-targets) * (1 - alpha)\n",
    "    loss = (bce * alpha * (1 - p) ** gamma)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f292f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(student,teacher,T=8):\n",
    "    loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    student = torch.nn.functional.log_softmax(student/T)\n",
    "    teacher = torch.nn.functional.softmax(teacher/T)\n",
    "\n",
    "    \n",
    "    return loss(student,teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "948eabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_kl_loss(student,teacher,targets,weight=0.5):\n",
    "    kl = kl_loss(student,teacher)\n",
    "    student = torch.masked_select(student,targets!=-1)\n",
    "    targets = torch.masked_select(targets,targets!=-1)\n",
    "    if targets.size(0) > 0:\n",
    "\n",
    "        focal = focal_loss(sigmoid(student),targets)\n",
    "    else:\n",
    "        focal = kl\n",
    "    return focal * weight + kl * (1-weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74ac49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_combination(p_loss,a_loss,alpha=0.5):\n",
    "    return p_loss * alpha + a_loss * (1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ff06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion_params_a,criterion_params_b,lamb,criterion):\n",
    "    return criterion(*criterion_params_a) * lamb + criterion(*criterion_params_b) * (1-lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1ecc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = combined_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:33.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:06.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:39.\n",
      "\n",
      "  Average training loss: 0.012773506741468925\n",
      "  Average primary loss: 0.007991020633462681\n",
      "  Average aux loss: 7.840731996111572e-05\n",
      "  Training epoch took: 0:05:59\n",
      "  Testing primary...\n",
      "  AUC primária: 0.13955542741388183\n",
      "  Average test primary loss: 0.010306467214282954\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.07120043504506529\n",
      "  Average test loss: 0.04092991093771075\n",
      "  Average test aux loss: 0.07155335466113855\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.010057473360980654\n",
      "  Average primary loss: 0.004716574000267051\n",
      "  Average aux loss: 7.063095836201683e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.2625204310389268\n",
      "  Average test primary loss: 0.008594012327211083\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.16316033008668404\n",
      "  Average test loss: 0.03613697908344274\n",
      "  Average test aux loss: 0.0636799458396744\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.009507695911452174\n",
      "  Average primary loss: 0.003993575738675391\n",
      "  Average aux loss: 6.964640488149598e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.34516602730800533\n",
      "  Average test primary loss: 0.008525640888245038\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.23313751177388067\n",
      "  Average test loss: 0.03330399381119828\n",
      "  Average test aux loss: 0.058082346734151524\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.008950795627252194\n",
      "  Average primary loss: 0.003419344441921793\n",
      "  Average aux loss: 9.765219147084281e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.35969524449839774\n",
      "  Average test primary loss: 0.007757307733145525\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.26442542455291934\n",
      "  Average test loss: 0.031096901538131654\n",
      "  Average test aux loss: 0.05443649534311778\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.008313133301686284\n",
      "  Average primary loss: 0.0028196725389049173\n",
      "  Average aux loss: 6.705034320475534e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3593313398670501\n",
      "  Average test primary loss: 0.008682322253490676\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.257725982776983\n",
      "  Average test loss: 0.031352426156067766\n",
      "  Average test aux loss: 0.05402253005864485\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.007580795414610693\n",
      "  Average primary loss: 0.002314415940993171\n",
      "  Average aux loss: 5.8877460105577484e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.36483255600805076\n",
      "  Average test primary loss: 0.00988547848361843\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.2554741995524733\n",
      "  Average test loss: 0.03385278141512623\n",
      "  Average test aux loss: 0.057820084346634035\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.00711793399662011\n",
      "  Average primary loss: 0.002103492419007403\n",
      "  Average aux loss: 5.35353938175831e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.35516625266710083\n",
      "  Average test primary loss: 0.010875128670859168\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.26089614055128957\n",
      "  Average test loss: 0.03481859676452037\n",
      "  Average test aux loss: 0.05876206485818158\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:33.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.006822537853976065\n",
      "  Average primary loss: 0.0019729913396632943\n",
      "  Average aux loss: 6.839061825303361e-05\n",
      "  Training epoch took: 0:06:00\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3574536435245241\n",
      "  Average test primary loss: 0.010619720439690183\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.27502114019497786\n",
      "  Average test loss: 0.035711761298237964\n",
      "  Average test aux loss: 0.06080380215678575\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:33.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.006595407833388664\n",
      "  Average primary loss: 0.0019058523968964757\n",
      "  Average aux loss: 5.306803359417245e-05\n",
      "  Training epoch took: 0:06:00\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3408742032872066\n",
      "  Average test primary loss: 0.012165521350601371\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.27526094388394234\n",
      "  Average test loss: 0.037292262310829924\n",
      "  Average test aux loss: 0.06241900327105848\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:33.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.006317428558090113\n",
      "  Average primary loss: 0.0018576079654722074\n",
      "  Average aux loss: 5.829547444591299e-05\n",
      "  Training epoch took: 0:06:00\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3578953295543679\n",
      "  Average test primary loss: 0.011597895933280014\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.2818376654157165\n",
      "  Average test loss: 0.03748705370773404\n",
      "  Average test aux loss: 0.06337621148218806\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:33.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.006264767114922327\n",
      "  Average primary loss: 0.0018070738480928223\n",
      "  Average aux loss: 3.9167276554508135e-05\n",
      "  Training epoch took: 0:06:00\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3528528016565041\n",
      "  Average test primary loss: 0.011180803575113698\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.24724038552728334\n",
      "  Average test loss: 0.038075475399997435\n",
      "  Average test aux loss: 0.06497014722488118\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:40.\n",
      "\n",
      "  Average training loss: 0.006127146592030755\n",
      "  Average primary loss: 0.0018060937055013832\n",
      "  Average aux loss: 5.696051084669307e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3636049148595837\n",
      "  Average test primary loss: 0.011197310969022647\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.2658630789327418\n",
      "  Average test loss: 0.03883619212440022\n",
      "  Average test aux loss: 0.0664750732797778\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.006032092548626924\n",
      "  Average primary loss: 0.0017374955763717042\n",
      "  Average aux loss: 7.023163925623521e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3743527735849709\n",
      "  Average test primary loss: 0.01169977577860063\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.27040489319647887\n",
      "  Average test loss: 0.03789053309076237\n",
      "  Average test aux loss: 0.06408129040292411\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.006025023062935417\n",
      "  Average primary loss: 0.001765633027785255\n",
      "  Average aux loss: 5.9270478232065216e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.3672477218422356\n",
      "  Average test primary loss: 0.012393299151950006\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.27056002800056783\n",
      "  Average test loss: 0.03961453987461216\n",
      "  Average test aux loss: 0.06683578059727431\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n",
      "  Batch   200  of    386.    Elapsed: 0:03:07.\n",
      "  Batch   300  of    386.    Elapsed: 0:04:41.\n",
      "\n",
      "  Average training loss: 0.005896423589269307\n",
      "  Average primary loss: 0.001722249578895081\n",
      "  Average aux loss: 5.673586929333396e-05\n",
      "  Training epoch took: 0:06:01\n",
      "  Testing primary...\n",
      "  AUC primária: 0.39066879255361053\n",
      "  Average test primary loss: 0.011945243454801868\n",
      "  Testing auxiliary...\n",
      "  AUC auxiliar: 0.28587345701336914\n",
      "  Average test loss: 0.040183575564714254\n",
      "  Average test aux loss: 0.06842190767462664\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/2169368882.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  student = torch.nn.functional.log_softmax(student/T)\n",
      "/tmp/ipykernel_8571/2169368882.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  teacher = torch.nn.functional.softmax(teacher/T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    386.    Elapsed: 0:01:34.\n"
     ]
    }
   ],
   "source": [
    "model_vars = [i[1] for i in combined_model.named_parameters() if i[0].find(\"alpha\") == -1 and i[0].find(\"beta\") == -1]\n",
    "alpha_vars = [i[1] for i in combined_model.named_parameters() if i[0].find(\"alpha\") != -1 or i[0].find(\"beta\") != -1] \n",
    "\n",
    "\n",
    "prim_optimizer = torch.optim.AdamW([{\"params\":model_vars},{\"params\":alpha_vars,\"lr\":0.001}],lr=learning_rate)\n",
    "sigmoid = torch.sigmoid\n",
    "aux_optimizer = torch.optim.AdamW([{\"params\":model_vars},{\"params\":alpha_vars,\"lr\":0.001}],lr=learning_rate)\n",
    "\n",
    "\n",
    "if apply_scheduler:\n",
    "    num_train_steps = int(len(aux_train_dataloader) * num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "    prim_scheduler = get_constant_schedule_with_warmup(prim_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "    \n",
    "    aux_scheduler = get_constant_schedule_with_warmup(aux_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "    \n",
    "\n",
    "sigmoid = torch.sigmoid\n",
    "\n",
    "\n",
    "    \n",
    "train_loss_func = focal_kl_loss\n",
    "loss_func = focal_loss\n",
    "multitask_loss = naive_combination\n",
    "\n",
    "best_loss_prim = np.inf\n",
    "\n",
    "best_auc_prim = 0\n",
    "\n",
    "best_loss_aux = np.inf\n",
    "\n",
    "best_auc_aux = 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss_batch_train(batch,model,criterion):\n",
    "    \n",
    "    input_ids = batch[0].to(device)\n",
    "    input_masks = batch[1].to(device)\n",
    "    logits = batch[2].to(device)\n",
    "    labels = batch[3].to(device)\n",
    "\n",
    "    task_idx = int(batch[4][0][0].item())\n",
    "    \n",
    "    #print(batch)\n",
    "    \n",
    "    if model.training:\n",
    "        \n",
    "        loss_f = lambda x,y,z: mixup_criterion(x,y,z,criterion=criterion)\n",
    "\n",
    "        outputs,labels_a,labels_b,lamb = model(input_ids,input_masks,labels,task_idx)\n",
    "\n",
    "        return loss_f((outputs,logits,labels_a),(outputs,logits,labels_b),lamb)\n",
    "    else:\n",
    "        #acho que isso aqui nao importa mais\n",
    "        outputs = model(input_ids,input_masks,labels,task_idx)\n",
    "        return criterion(sigmoid(outputs),labels), outputs,labels\n",
    "    \n",
    "    \n",
    "def calculate_loss_batch(batch,model,criterion):\n",
    "    \n",
    "    input_ids = batch[0].to(device)\n",
    "    input_masks = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "\n",
    "    task_idx = int(batch[3][0][0].item())\n",
    "    \n",
    "    \n",
    "    if model.training:\n",
    "        \n",
    "        loss_f = lambda x,y,z: mixup_criterion(x,y,z,criterion=criterion)\n",
    "\n",
    "        outputs,labels_a,labels_b,lamb = model(input_ids,input_masks,labels,task_idx)\n",
    "\n",
    "        return loss_f((sigmoid(outputs),labels_a),(sigmoid(outputs),labels_b),lamb)\n",
    "    else:\n",
    "        \n",
    "        outputs = model(input_ids,input_masks,labels,task_idx)\n",
    "        return criterion(sigmoid(outputs),labels), outputs,labels    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for epoch_i in range(0,num_train_epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tr_loss = 0\n",
    "    pr_loss = 0\n",
    "    aux_loss = 0\n",
    "    \n",
    "    combined_model.train()\n",
    "    prim_iter = iter(prim_train_dataloader)\n",
    "    aux_iter = iter(aux_train_dataloader)\n",
    "    \n",
    "    #no caso tô assumindo que o aux é sempre maior, porque esse é meu caso agora\n",
    "    \n",
    "    for step,aux_batch in enumerate(aux_iter):\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(aux_train_dataloader), elapsed))\n",
    "        try:\n",
    "            prim_batch = prim_iter.__next__()\n",
    "        except StopIteration:\n",
    "            prim_iter = iter(prim_train_dataloader)\n",
    "            prim_batch = prim_iter.__next__()\n",
    "        \n",
    "        prim_loss = calculate_loss_batch_train(prim_batch,combined_model,train_loss_func)\n",
    "        prim_optimizer.zero_grad()\n",
    "        prim_loss.backward()\n",
    "        prim_optimizer.step()\n",
    "        \n",
    "        if apply_scheduler:\n",
    "            prim_scheduler.step()\n",
    "        \n",
    "        \n",
    "        aux_loss = calculate_loss_batch_train(aux_batch,combined_model,train_loss_func)\n",
    "        \n",
    "        aux_optimizer.zero_grad()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "        \n",
    "        if apply_scheduler:\n",
    "            aux_scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_loss = multitask_loss(prim_loss,aux_loss)\n",
    "        \n",
    "\n",
    "            \n",
    "            tr_loss += batch_loss.item()\n",
    "        \n",
    "            pr_loss += prim_loss.item()\n",
    "        \n",
    "            aux_loss += aux_loss.item()\n",
    "\n",
    "        \n",
    "    \n",
    "    avg_prim_loss = pr_loss/len(aux_train_dataloader)    \n",
    "    avg_train_loss = tr_loss/len(aux_train_dataloader)\n",
    "    avg_aux_loss = aux_loss/len(aux_train_dataloader)    \n",
    "\n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {:}\".format(avg_train_loss))\n",
    "    print(\"  Average primary loss: {:}\".format(avg_prim_loss))\n",
    "    print(\"  Average aux loss: {:}\".format(avg_aux_loss))\n",
    "\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    \n",
    "    print(\"  Testing primary...\")\n",
    "    combined_model.eval()\n",
    "    \n",
    "    pr_loss = 0\n",
    "    tt_loss = 0\n",
    "    aux_loss = 0\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels_ids = []\n",
    "    \n",
    "    for prim_batch in prim_test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            prim_loss,outputs,labels = calculate_loss_batch(prim_batch,combined_model,loss_func)\n",
    "            probs = sigmoid(outputs)\n",
    "            pr_loss += prim_loss.item()\n",
    "            \n",
    "        all_probs += probs.detach().cpu()\n",
    "        all_labels_ids += labels.detach().cpu()\n",
    "        \n",
    "    all_probs = torch.stack(all_probs).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    \n",
    "    cur_auc = average_precision_score(all_labels_ids,all_probs)\n",
    "\n",
    "    \n",
    "    avg_prim_test_loss = pr_loss/len(prim_test_dataloader)\n",
    "    \n",
    "    if cur_auc > best_auc_prim:\n",
    "        best_auc_prim = cur_auc\n",
    "        \n",
    "    if avg_prim_test_loss < best_loss_prim:\n",
    "        best_loss = avg_prim_test_loss\n",
    "        best_probs_prim = all_probs\n",
    "        best_labels_prim = all_labels_ids\n",
    "        \n",
    "    print(\"  AUC primária:\",cur_auc)\n",
    "    print(\"  Average test primary loss: {:}\".format(avg_prim_test_loss))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"  Testing auxiliary...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    aux_loss = 0\n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels_ids = []\n",
    "    \n",
    "    for prim_batch in aux_test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            prim_loss,outputs,labels = calculate_loss_batch(prim_batch,combined_model,loss_func)\n",
    "            probs = sigmoid(outputs)\n",
    "            aux_loss += prim_loss.item()\n",
    "            \n",
    "        all_probs += probs.detach().cpu()\n",
    "        all_labels_ids += labels.detach().cpu()\n",
    "        \n",
    "    all_probs = torch.stack(all_probs).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    \n",
    "    cur_auc = average_precision_score(all_labels_ids,all_probs)\n",
    "\n",
    "    \n",
    "    avg_aux_test_loss = aux_loss/len(prim_test_dataloader)  \n",
    "    \n",
    "    if cur_auc > best_auc_aux:\n",
    "        best_auc_aux = cur_auc\n",
    "        \n",
    "    if avg_aux_test_loss < best_loss_aux:\n",
    "        best_loss = avg_aux_test_loss\n",
    "        best_probs_aux = all_probs\n",
    "        best_labels_aux = all_labels_ids\n",
    "        \n",
    "    print(\"  AUC auxiliar:\",cur_auc)\n",
    "    avg_test_loss = multitask_loss(avg_prim_test_loss,avg_aux_test_loss)\n",
    "    \n",
    "    print(\"  Average test loss: {:}\".format(avg_test_loss))\n",
    "    print(\"  Average test aux loss: {:}\".format(avg_aux_test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328fbfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sharing_layer in combined_model.soft_sharing_layers:\n",
    "    print(torch.tensor([[sharing_layer.alpha_prim,sharing_layer.beta_prim],[sharing_layer.beta_aux,sharing_layer.alpha_aux]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f7e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in range (best_probs_prim.shape[1]):\n",
    "\n",
    "    precision,recall,thresholds = precision_recall_curve(best_labels_prim[:,i],best_probs_prim[:,i])\n",
    "\n",
    "    f1_scores = 2*recall*precision/(recall+precision)\n",
    "\n",
    "    cur_threshold = thresholds[np.nanargmax(f1_scores)]\n",
    "\n",
    "    print(\"melhor f1 para \",train_df.columns[i+1],\" \",np.nanmax(f1_scores))\n",
    "\n",
    "    preds.append(best_probs_prim[:,i] >= cur_threshold)\n",
    "\n",
    "    \n",
    "\n",
    "best_preds = np.array(preds).T\n",
    "\n",
    "best_fine_tuning_report = classification_report(best_labels_prim,best_preds,target_names=train_df.columns[1:], zero_division=0,output_dict=True)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.heatmap(pd.DataFrame(best_fine_tuning_report).iloc[:-1, :].T, annot=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganbert",
   "language": "python",
   "name": "ganbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
