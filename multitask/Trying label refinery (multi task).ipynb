{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0708ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel,AutoConfig,get_constant_schedule_with_warmup, BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score,f1_score,average_precision_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1264e8df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0,is_loss=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.is_loss = is_loss\n",
    "        if is_loss:\n",
    "            self.min_validation_loss = np.inf\n",
    "        else:\n",
    "            self.min_validation_loss = -np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss,save_best=False,model=None,model_path=\"temp\"):\n",
    "        if self.is_loss:\n",
    "            if validation_loss < self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW LOWEST LOSS \",self.min_validation_loss)\n",
    "                if save_best:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        else:\n",
    "            if validation_loss > self.min_validation_loss:\n",
    "                self.min_validation_loss = validation_loss\n",
    "                self.counter = 0\n",
    "                print(\"NEW HIGHEST SCORE \",self.min_validation_loss)\n",
    "                if save_best:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            elif validation_loss <= (self.min_validation_loss + self.min_delta):\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369a1900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38aeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dessa vez sem fixar a seed, exceto para a divisão dos dados\n",
    "seed_val = 2023\n",
    "if torch.cuda.is_available():\n",
    "  device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6ff489",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "remove_neutral_instances = False\n",
    "\n",
    "\n",
    "learning_rate =  5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 15\n",
    "multi_gpu = True\n",
    "filter_emotions = False\n",
    "\n",
    "apply_scheduler = True\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "T = 8\n",
    "\n",
    "print_each_n_step = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "#model_path = \"models/go_emotions\"\n",
    "\n",
    "alpha = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63324d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../features/both tasks facebook + reddit.csv\",index_col = 0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cdc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {\n",
    "    \"admiração\": 0,\n",
    "    \"diversão\": 1,\n",
    "    \"raiva\": 2,\n",
    "    \"aborrecimento\": 3,\n",
    "    \"aprovação\": 4,\n",
    "    \"zelo\": 5,\n",
    "    \"confusão\": 6,\n",
    "    \"curiosidade\": 7,\n",
    "    \"desejo\": 8,\n",
    "    \"decepção\": 9,\n",
    "    \"desaprovação\": 10,\n",
    "    \"nojo\": 11,\n",
    "    \"constrangimento\": 12,\n",
    "    \"entusiasmo\": 13,\n",
    "    \"medo\": 14,\n",
    "    \"gratidão\": 15,\n",
    "    \"luto\": 16,\n",
    "    \"alegria\": 17,\n",
    "    \"amor\": 18,\n",
    "    \"nervosismo\": 19,\n",
    "    \"otimismo\": 20,\n",
    "    \"orgulho\": 21,\n",
    "    \"percepção\": 22,\n",
    "    \"alívio\": 23,\n",
    "    \"remorso\": 24,\n",
    "    \"tristeza\": 25,\n",
    "    \"surpresa\": 26,\n",
    "    \"neutro\": 27\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b97e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_num = len(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2da5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = [\"neutro\", \"raiva\", \"zelo\",\"confusão\", \"desejo\",\"decepção\",\"desaprovação\",\"gratidão\",\"luto\",\"amor\", \"tristeza\"]\n",
    "#to_keep = ['medo', 'tristeza','raiva','zelo']\n",
    "\n",
    "if filter_emotions:\n",
    "    drop=  []\n",
    "    for key in emotion_dict:\n",
    "        if key in to_keep:\n",
    "            drop.append(key)\n",
    "\n",
    "    for key in drop:\n",
    "        del emotion_dict[key]\n",
    "    if filter_emotions:\n",
    "        idx = [str(emotion_dict[key])+\".1\" if emotion_dict[key] < symptom_num else str(emotion_dict[key]) for key in emotion_dict ]\n",
    "        train_df = train_df.drop(idx,axis=1)\n",
    "    emotion_num = len(drop)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546f69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/segredos_sentenças_multitask_clean.csv\",index_col=0)\n",
    "\n",
    "df1 = df1.drop([\"Postagem com possível perfil depressivo\",\"Alteração na eficiência\",\n",
    "         \"Alteração da funcionalidade\",\"*\",'Agitação/inquietação','Sintoma obsessivo e compulsivo','Déficit de atenção/Memória',\n",
    "              'Perda/Diminuição do prazer/ Perda/Diminuição da libido'],axis=1)\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "#df1 = df1[df1.iloc[:,1:].sum(axis=1) != 0]\n",
    "\n",
    "#df1 = df1.reset_index(drop=True)\n",
    "symptom_num = df1.iloc[:,1:].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9719dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e74174d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def build_dataset(df,df1):\n",
    "    symptoms = df1.iloc[:,1:]\n",
    "    strat = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=seed_val).split(df1,symptoms)\n",
    "    for train_idx, test_idx in strat:\n",
    "\n",
    "        symptoms_train = symptoms.loc[train_idx]\n",
    "        symptoms_test  = symptoms.loc[test_idx]\n",
    "        train_df = df.loc[train_idx]\n",
    "        test_df = df.loc[test_idx]\n",
    "        break\n",
    "    remaining = df.loc[~df.index.isin(train_idx)]\n",
    "    remaining = remaining.loc[~remaining.index.isin(test_idx)]\n",
    "\n",
    "    train_remaining,test_remaining, _, _ = train_test_split(remaining,remaining,test_size=0.2,shuffle=True,random_state=seed_val)\n",
    "\n",
    "    train_df = pd.concat([train_df,train_remaining]).reset_index(drop=True)\n",
    "    test_df = pd.concat([test_df,test_remaining]).reset_index(drop=True)\n",
    "    return train_df,test_df,symptoms_train,symptoms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696a53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def get_dataloader(df,tokenizer,labels,batch_size,do_shuffle):\n",
    "    examples = []\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in df.text:\n",
    "        encoded = tokenizer.encode_plus(text,return_attention_mask=True,add_special_tokens=True,max_length = max_length,\n",
    "    padding=\"max_length\",truncation=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    df = df.drop(\"text\",axis=1)\n",
    "\n",
    "    logits = torch.tensor(df.iloc[:,:symptom_num + emotion_num].to_numpy(),dtype=torch.float32)\n",
    "    \n",
    "    labels = torch.tensor(np.concatenate((labels.to_numpy(),torch.nn.functional.sigmoid(logits[labels.shape[0]:,:symptom_num]))),dtype=torch.float32)\n",
    "    labeled = torch.tensor(np.concatenate((np.ones(labels.size()[0]), np.zeros(logits.size()[0] - labels.size()[0]))))\n",
    "    # modificando logits\n",
    "    #with torch.no_grad():\n",
    "    #    logits[:labels.size()[0],:symptom_num] = (logits[:labels.size()[0],:symptom_num] + labels)/2\n",
    "    \n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(input_ids,attention_masks,logits,labeled,labels)\n",
    "    \n",
    "    if do_shuffle:\n",
    "        sampler = torch.utils.data.RandomSampler\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler\n",
    "    return torch.utils.data.DataLoader(dataset,sampler = sampler(dataset),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e6b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multitask(nn.Module):\n",
    "    def __init__(self,num_primary_labels, num_auxiliary_labels,transformer,input_size=512):\n",
    "        super(Multitask,self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.primary_head = nn.Linear(input_size,num_primary_labels)\n",
    "        self.auxiliary_head = nn.Linear(input_size,num_auxiliary_labels)\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        \n",
    "    def forward(self,input_ids,input_mask):\n",
    "        input_arr = transformer(input_ids,attention_mask=input_mask)[-1]\n",
    "        primary_logits = self.primary_head(input_arr)\n",
    "        auxiliary_logits = self.auxiliary_head(input_arr)\n",
    "        \n",
    "        \n",
    "        return primary_logits,auxiliary_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748504d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "symptoms = df1.iloc[:,1:]\n",
    "strat = MultilabelStratifiedKFold(n_splits=2, shuffle=True, random_state=seed_val).split(df1,symptoms)\n",
    "\n",
    "for train_idx, test_idx in strat:\n",
    "\n",
    "    train_df = df1.loc[train_idx]\n",
    "    test_df  = df1.loc[test_idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff049e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df.iloc[:,1:]\n",
    "test_text = test_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "553403d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "attention_masks =[]\n",
    "\n",
    "for text in test_text:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "    inputs.append(encoding['input_ids'])\n",
    "    attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "labels = torch.tensor(test_labels.to_numpy(),dtype=torch.float32)    \n",
    "inputs = torch.stack(inputs)\n",
    "attention_masks = torch.stack(attention_masks)\n",
    "\n",
    "dataset = TensorDataset(inputs, attention_masks,labels)\n",
    "symptom_dataloader = DataLoader(\n",
    "              dataset,\n",
    "              sampler = SequentialSampler(dataset), \n",
    "              batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b98da",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc9dde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(p,targets,gamma=3,alpha=0.8):\n",
    "\n",
    "    bce = torch.nn.BCELoss()(p,targets)\n",
    "    p = torch.where(targets == 1,p,1-p)\n",
    "    alpha = targets * alpha + (1-targets) * (1-alpha)\n",
    "    return (alpha * (1-p) ** gamma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e70447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_loss(losses,var_logs):\n",
    "    total_loss = 0\n",
    "    for i,loss in enumerate(losses):\n",
    "        total_loss += loss * torch.exp(-var_logs[i]) + var_logs[i]\n",
    "    #total_loss += torch.prod(torch.tensor(var_logs))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "717e6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student,teacher,weights,b_labels,labeled_weight = 0.5):\n",
    "    loss = torch.nn.MSELoss(reduction='none')\n",
    "    student = torch.nn.functional.sigmoid(student)\n",
    "    teacher = torch.nn.functional.sigmoid(teacher)\n",
    "    \n",
    "    if b_labels != None:\n",
    "        teacher = (teacher * (labeled_weight) + b_labels * (1-labeled_weight))\n",
    "\n",
    "    l = loss(student,teacher)\n",
    "    \n",
    "    \n",
    "    #l = l + (l.T * (weights * labeled_weight)).T\n",
    "    return l.mean()/student.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fe6fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_distill(student,teacher,weights,b_labels,T=3):\n",
    "    loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    student = torch.nn.functional.log_softmax(student/T)\n",
    "    teacher = torch.nn.functional.softmax(teacher/T)\n",
    "    return loss(student,teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b9574d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_combination(p_loss,a_loss,alpha=0.5):\n",
    "    return p_loss * alpha + a_loss * (1-alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b7dab",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb8f5e",
   "metadata": {},
   "source": [
    "We have to try really hard to not overfit the model in order for label refinery to work. So I'm going to try:\n",
    "\n",
    "* Higher dropout values\n",
    "* Mixup\n",
    "* Maybe START (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f86cdced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_dataloader,symptom_dataloader,primary_loss,auxiliary_loss):\n",
    "\n",
    "    stop = EarlyStopper(is_loss = False,patience=4)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    log_var_a = torch.zeros((1,), requires_grad=True,device=device)\n",
    "    log_var_b = torch.zeros((1,), requires_grad=True,device=device)\n",
    "\n",
    "    log_vars = [log_var_a,log_var_b]\n",
    "    model_vars = [i for i in model.parameters()]\n",
    "    optimizer =  torch.optim.AdamW([{'params':model_vars},{'params':log_vars,'lr':0.001}], lr= learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    primary_loss_func = primary_loss\n",
    "    auxiliary_loss_func = auxiliary_loss\n",
    "    multitask_loss_func = naive_combination\n",
    "\n",
    "    sigmoid = torch.sigmoid\n",
    "    best_f1 = 0\n",
    "    best_epoch = 0\n",
    "    best_report = {}\n",
    "    best_auc = 0\n",
    "\n",
    "\n",
    "    for epoch_i in range(0,num_train_epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "\n",
    "        tr_loss = 0\n",
    "        pr_loss = 0\n",
    "        aux_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        all_probs = []\n",
    "        all_labels_ids = []\n",
    "        labeled_num = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                #print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_logits = batch[2].to(device)\n",
    "            b_labeled = batch[3].to(device)\n",
    "            b_labels = batch[4].to(device)\n",
    "            \n",
    "            b_primary_logits = b_logits[:,:symptom_num]\n",
    "            b_auxiliary_logits = b_logits[:,symptom_num:]\n",
    "\n",
    "            cur_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "\n",
    "            prim_outputs, aux_outputs = model(b_input_ids,b_input_mask)\n",
    "\n",
    "\n",
    "            #output,targets_a,targets_b,lam = mixup(output.logits,b_labels)\n",
    "\n",
    "\n",
    "            temp_func_prim = lambda x,y : primary_loss_func(x,y,b_labeled,b_labels)\n",
    "            temp_func_aux = lambda x,y : auxiliary_loss_func(x,y,torch.zeros_like(b_labeled),None)\n",
    "\n",
    "\n",
    "            losses = [temp_func_prim(prim_outputs,b_primary_logits),temp_func_aux(aux_outputs,b_auxiliary_logits)]\n",
    "            #batch_loss = multitask_loss_func(losses,variance_vars)\n",
    "            batch_loss = multitask_loss_func(losses[0],losses[1])\n",
    "            tr_loss += batch_loss.item()\n",
    "            pr_loss += losses[0].item()\n",
    "            aux_loss += losses[1].item()\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        avg_train_loss = tr_loss/len(train_dataloader)\n",
    "        avg_prim_loss = pr_loss/len(train_dataloader)\n",
    "        avg_aux_loss = aux_loss/len(train_dataloader)\n",
    "\n",
    "\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "        print(\"  Average primary loss: {0:.3f}\".format(avg_prim_loss))\n",
    "        print(\"  Average auxiliary loss: {0:.3f}\".format(avg_aux_loss))\n",
    "\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"  Labeled Num {:}\".format(labeled_num))\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "\n",
    "        all_probs = []\n",
    "        all_labels_ids = []\n",
    "        for batch in symptom_dataloader:\n",
    "\n",
    "            b_input_ids = batch[0].reshape(len(batch[0]),max_length).to(device)\n",
    "            b_input_mask = batch[1].reshape(len(batch[0]),max_length).to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            cur_batch_size = b_input_ids.shape[0]\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output,_ = model(b_input_ids,b_input_mask)\n",
    "                preds = torch.nn.functional.sigmoid(output)\n",
    "                loss = focal_loss(preds,b_labels)\n",
    "                for el in preds:\n",
    "                    all_probs.append(el.detach().cpu())\n",
    "                for el in b_labels:\n",
    "                    all_labels_ids.append(el.detach().cpu())\n",
    "\n",
    "\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        all_probs = torch.stack(all_probs).numpy()\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        cur_auc = average_precision_score(all_labels_ids,all_probs)\n",
    "\n",
    "        print(\"auc:\",cur_auc)    \n",
    "        avg_test_loss = total_test_loss/len(symptom_dataloader)\n",
    "\n",
    "\n",
    "        print(\"test Loss:\", avg_test_loss)\n",
    "\n",
    "\n",
    "        if stop.early_stop(cur_auc,save_best=True,model=model,model_path = \"../models/temp\"):\n",
    "            break\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Test Loss': avg_test_loss\n",
    "            })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2569799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(df,model):\n",
    "    inputs = df.text\n",
    "    logits = []\n",
    "    for i,example in enumerate(inputs):\n",
    "    \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"{i} de {len(inputs)}\")\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            example,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            mask = encoding['attention_mask'].to(device)\n",
    "            outputs_prim,outputs_aux = model(input_ids,mask)\n",
    "            outputs = torch.hstack([outputs_prim,outputs_aux])\n",
    "            logit = outputs.squeeze(-2)\n",
    "            del outputs\n",
    "            del outputs_prim\n",
    "            del outputs_aux\n",
    "            del input_ids\n",
    "            del mask\n",
    "            \n",
    "            logits.append(logit)\n",
    "            del logit\n",
    "    logits = np.array(torch.stack(logits).cpu())\n",
    "    df = pd.DataFrame(logits).reset_index(drop=True)\n",
    "    df['text'] = inputs.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63da5f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.001\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.050167426414922535\n",
      "test Loss: 0.011903780197875725\n",
      "NEW HIGHEST SCORE  0.050167426414922535\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.10010514582448783\n",
      "test Loss: 0.010084602616828036\n",
      "NEW HIGHEST SCORE  0.10010514582448783\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.10743382130448001\n",
      "test Loss: 0.009975787000478926\n",
      "NEW HIGHEST SCORE  0.10743382130448001\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.14792573110644555\n",
      "test Loss: 0.008587632966982955\n",
      "NEW HIGHEST SCORE  0.14792573110644555\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.21088663146931924\n",
      "test Loss: 0.00750949903630425\n",
      "NEW HIGHEST SCORE  0.21088663146931924\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.23592696686376588\n",
      "test Loss: 0.007475398159963456\n",
      "NEW HIGHEST SCORE  0.23592696686376588\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.27127565168855133\n",
      "test Loss: 0.006752771734436219\n",
      "NEW HIGHEST SCORE  0.27127565168855133\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2707415446888517\n",
      "test Loss: 0.006722695622089747\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2891110825974984\n",
      "test Loss: 0.007160852694682576\n",
      "NEW HIGHEST SCORE  0.2891110825974984\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.30769079818406114\n",
      "test Loss: 0.0067896150128060094\n",
      "NEW HIGHEST SCORE  0.30769079818406114\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.32682168265140243\n",
      "test Loss: 0.006752081542603068\n",
      "NEW HIGHEST SCORE  0.32682168265140243\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3168695908105803\n",
      "test Loss: 0.007190120510278722\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3326256210341485\n",
      "test Loss: 0.006773618751557896\n",
      "NEW HIGHEST SCORE  0.3326256210341485\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3300297236034917\n",
      "test Loss: 0.006845176433892669\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.33898308249166503\n",
      "test Loss: 0.006881717973187365\n",
      "NEW HIGHEST SCORE  0.33898308249166503\n",
      "melhor f1 para  1   0.6896551724137931\n",
      "melhor f1 para  2   0.35616438356164387\n",
      "melhor f1 para  3   0.4985337243401759\n",
      "melhor f1 para  4   0.4644549763033175\n",
      "melhor f1 para  5   0.6877828054298643\n",
      "melhor f1 para  6   0.5981308411214953\n",
      "melhor f1 para  7   0.34782608695652173\n",
      "melhor f1 para  8   0.484472049689441\n",
      "melhor f1 para  9   0.3623693379790941\n",
      "melhor f1 para  10   0.1967213114754098\n",
      "melhor f1 para  11   0.380952380952381\n",
      "melhor f1 para  12   0.34285714285714286\n",
      "melhor f1 para  13   0.4\n",
      "melhor f1 para  14   0.4383561643835616\n",
      "melhor f1 para  15   0.15384615384615383\n",
      "melhor f1 para  16   0.37623762376237624\n",
      "melhor f1 para  17   0.2580645161290323\n",
      "melhor f1 para  text   0.3125\n",
      "average precision score:  0.33898308249166503\n",
      "NEW HIGHEST SCORE  0.33898308249166503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 de 6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.001\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.0613477837617251\n",
      "test Loss: 0.012517206056194531\n",
      "NEW HIGHEST SCORE  0.0613477837617251\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.09130786432815573\n",
      "test Loss: 0.01037988529817478\n",
      "NEW HIGHEST SCORE  0.09130786432815573\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.0941623031053843\n",
      "test Loss: 0.010411896790075745\n",
      "NEW HIGHEST SCORE  0.0941623031053843\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.1339969310773229\n",
      "test Loss: 0.009324186371412833\n",
      "NEW HIGHEST SCORE  0.1339969310773229\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.19192547637481022\n",
      "test Loss: 0.008468972034500661\n",
      "NEW HIGHEST SCORE  0.19192547637481022\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.236912194522788\n",
      "test Loss: 0.008649406885426189\n",
      "NEW HIGHEST SCORE  0.236912194522788\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2783359911246432\n",
      "test Loss: 0.0076057535935998765\n",
      "NEW HIGHEST SCORE  0.2783359911246432\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2766882357417132\n",
      "test Loss: 0.007546170945027591\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2980180036957314\n",
      "test Loss: 0.007587498692619438\n",
      "NEW HIGHEST SCORE  0.2980180036957314\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.30053793513142446\n",
      "test Loss: 0.007470408895027799\n",
      "NEW HIGHEST SCORE  0.30053793513142446\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.30107238035817807\n",
      "test Loss: 0.007806019190497495\n",
      "NEW HIGHEST SCORE  0.30107238035817807\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3047284565521571\n",
      "test Loss: 0.007587511045228992\n",
      "NEW HIGHEST SCORE  0.3047284565521571\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3125555224598498\n",
      "test Loss: 0.007604441026578078\n",
      "NEW HIGHEST SCORE  0.3125555224598498\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.3271699315931216\n",
      "test Loss: 0.007501936731608333\n",
      "NEW HIGHEST SCORE  0.3271699315931216\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.32353802267901005\n",
      "test Loss: 0.007476456252530821\n",
      "melhor f1 para  1   0.64\n",
      "melhor f1 para  2   0.3582089552238806\n",
      "melhor f1 para  3   0.47583643122676583\n",
      "melhor f1 para  4   0.496551724137931\n",
      "melhor f1 para  5   0.6434782608695652\n",
      "melhor f1 para  6   0.6666666666666667\n",
      "melhor f1 para  7   0.36206896551724144\n",
      "melhor f1 para  8   0.47619047619047616\n",
      "melhor f1 para  9   0.3502304147465438\n",
      "melhor f1 para  10   0.23636363636363636\n",
      "melhor f1 para  11   0.42857142857142855\n",
      "melhor f1 para  12   0.2857142857142857\n",
      "melhor f1 para  13   0.46511627906976744\n",
      "melhor f1 para  14   0.4172661870503597\n",
      "melhor f1 para  15   0.09523809523809525\n",
      "melhor f1 para  16   0.4271844660194175\n",
      "melhor f1 para  17   0.2580645161290323\n",
      "melhor f1 para  18   0.27272727272727276\n",
      "average precision score:  0.3271699315931216\n",
      "0 de 6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.001\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.06523629363771334\n",
      "test Loss: 0.012211862504733977\n",
      "NEW HIGHEST SCORE  0.06523629363771334\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.07605707085192753\n",
      "test Loss: 0.011118159063059735\n",
      "NEW HIGHEST SCORE  0.07605707085192753\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.07999431907272961\n",
      "test Loss: 0.011034383407063983\n",
      "NEW HIGHEST SCORE  0.07999431907272961\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.08296923144900582\n",
      "test Loss: 0.011045623768901301\n",
      "NEW HIGHEST SCORE  0.08296923144900582\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.09361440915443914\n",
      "test Loss: 0.011136497197848922\n",
      "NEW HIGHEST SCORE  0.09361440915443914\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.13743204531749342\n",
      "test Loss: 0.00999741949065513\n",
      "NEW HIGHEST SCORE  0.13743204531749342\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.18553815227274587\n",
      "test Loss: 0.008477297073507027\n",
      "NEW HIGHEST SCORE  0.18553815227274587\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.20587794617705937\n",
      "test Loss: 0.00861060108385376\n",
      "NEW HIGHEST SCORE  0.20587794617705937\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.23822293635175149\n",
      "test Loss: 0.008423955289873521\n",
      "NEW HIGHEST SCORE  0.23822293635175149\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2525281715378177\n",
      "test Loss: 0.008565673138946295\n",
      "NEW HIGHEST SCORE  0.2525281715378177\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.262273209866464\n",
      "test Loss: 0.007910720714544123\n",
      "NEW HIGHEST SCORE  0.262273209866464\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.27977301306504254\n",
      "test Loss: 0.00786321163743835\n",
      "NEW HIGHEST SCORE  0.27977301306504254\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.28199497355252423\n",
      "test Loss: 0.008061567727300161\n",
      "NEW HIGHEST SCORE  0.28199497355252423\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.28834279129955714\n",
      "test Loss: 0.008138557987262469\n",
      "NEW HIGHEST SCORE  0.28834279129955714\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.28341715364691844\n",
      "test Loss: 0.008643563221699584\n",
      "melhor f1 para  1   0.5555555555555555\n",
      "melhor f1 para  2   0.2921348314606742\n",
      "melhor f1 para  3   0.4806201550387597\n",
      "melhor f1 para  4   0.4680851063829787\n",
      "melhor f1 para  5   0.6607929515418502\n",
      "melhor f1 para  6   0.6346153846153846\n",
      "melhor f1 para  7   0.33093525179856115\n",
      "melhor f1 para  8   0.5\n",
      "melhor f1 para  9   0.35294117647058826\n",
      "melhor f1 para  10   0.1967213114754098\n",
      "melhor f1 para  11   0.23529411764705882\n",
      "melhor f1 para  12   0.25531914893617025\n",
      "melhor f1 para  13   0.4848484848484848\n",
      "melhor f1 para  14   0.3973509933774834\n",
      "melhor f1 para  15   0.10526315789473685\n",
      "melhor f1 para  16   0.3692307692307692\n",
      "melhor f1 para  17   0.20895522388059704\n",
      "melhor f1 para  18   0.25\n",
      "average precision score:  0.28834279129955714\n",
      "0 de 6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.001\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.058829625464973834\n",
      "test Loss: 0.012898132340302942\n",
      "NEW HIGHEST SCORE  0.058829625464973834\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.0865428788094092\n",
      "test Loss: 0.011507286753102735\n",
      "NEW HIGHEST SCORE  0.0865428788094092\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.08671426485410692\n",
      "test Loss: 0.012909259609374646\n",
      "NEW HIGHEST SCORE  0.08671426485410692\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.08948654837338889\n",
      "test Loss: 0.011033286132874924\n",
      "NEW HIGHEST SCORE  0.08948654837338889\n",
      "\n",
      "======== Epoch 5 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.1413986872412942\n",
      "test Loss: 0.009836515735809665\n",
      "NEW HIGHEST SCORE  0.1413986872412942\n",
      "\n",
      "======== Epoch 6 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.16342906759299683\n",
      "test Loss: 0.009535159818148491\n",
      "NEW HIGHEST SCORE  0.16342906759299683\n",
      "\n",
      "======== Epoch 7 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.21367919775768382\n",
      "test Loss: 0.008449374773615115\n",
      "NEW HIGHEST SCORE  0.21367919775768382\n",
      "\n",
      "======== Epoch 8 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.22951276839332177\n",
      "test Loss: 0.008183217777342006\n",
      "NEW HIGHEST SCORE  0.22951276839332177\n",
      "\n",
      "======== Epoch 9 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.250557929539712\n",
      "test Loss: 0.008174121401876816\n",
      "NEW HIGHEST SCORE  0.250557929539712\n",
      "\n",
      "======== Epoch 10 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2596595126032599\n",
      "test Loss: 0.008092440894452503\n",
      "NEW HIGHEST SCORE  0.2596595126032599\n",
      "\n",
      "======== Epoch 11 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2569267285120519\n",
      "test Loss: 0.00824646809377481\n",
      "\n",
      "======== Epoch 12 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.2791174984845888\n",
      "test Loss: 0.008135755345023967\n",
      "NEW HIGHEST SCORE  0.2791174984845888\n",
      "\n",
      "======== Epoch 13 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.27364047389856316\n",
      "test Loss: 0.008083388788273206\n",
      "\n",
      "======== Epoch 14 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:49\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.28239521971697523\n",
      "test Loss: 0.008382554153433523\n",
      "NEW HIGHEST SCORE  0.28239521971697523\n",
      "\n",
      "======== Epoch 15 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.000\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:49\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.28075563491135885\n",
      "test Loss: 0.008211268190722409\n",
      "melhor f1 para  1   0.5217391304347826\n",
      "melhor f1 para  2   0.3448275862068966\n",
      "melhor f1 para  3   0.5017667844522968\n",
      "melhor f1 para  4   0.49606299212598426\n",
      "melhor f1 para  5   0.6536585365853659\n",
      "melhor f1 para  6   0.6728971962616822\n",
      "melhor f1 para  7   0.36521739130434777\n",
      "melhor f1 para  8   0.48322147651006714\n",
      "melhor f1 para  9   0.36690647482014394\n",
      "melhor f1 para  10   0.14285714285714288\n",
      "melhor f1 para  11   0.13333333333333333\n",
      "melhor f1 para  12   0.14457831325301204\n",
      "melhor f1 para  13   0.46153846153846156\n",
      "melhor f1 para  14   0.42647058823529416\n",
      "melhor f1 para  15   0.05128205128205129\n",
      "melhor f1 para  16   0.37974683544303806\n",
      "melhor f1 para  17   0.18181818181818182\n",
      "melhor f1 para  18   0.18181818181818182\n",
      "average precision score:  0.28239521971697523\n",
      "0 de 6568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "/tmp/ipykernel_26778/3126488110.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_scores = 2*recall*precision/(recall+precision)\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.001\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.061598799115230946\n",
      "test Loss: 0.012693031499086804\n",
      "NEW HIGHEST SCORE  0.061598799115230946\n",
      "\n",
      "======== Epoch 2 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.001\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.07878511019273275\n",
      "test Loss: 0.012425855359037381\n",
      "NEW HIGHEST SCORE  0.07878511019273275\n",
      "\n",
      "======== Epoch 3 / 15 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.000\n",
      "  Average primary loss: 0.001\n",
      "  Average auxiliary loss: 0.000\n",
      "  Training epoch took: 0:00:48\n",
      "  Labeled Num 0\n",
      "\n",
      "Running Test...\n",
      "auc: 0.08225045842472671\n",
      "test Loss: 0.01145086404741616\n",
      "NEW HIGHEST SCORE  0.08225045842472671\n",
      "\n",
      "======== Epoch 4 / 15 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m train_df,test_df,symptoms_train,symptoms_test \u001b[38;5;241m=\u001b[39m build_dataset(df,df1)\n\u001b[1;32m     13\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(train_df,tokenizer,symptoms_train,batch_size,do_shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43msymptom_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclassic_distill\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclassic_distill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/temp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     17\u001b[0m probs \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, symptom_dataloader, primary_loss, auxiliary_loss)\u001b[0m\n\u001b[1;32m     78\u001b[0m     aux_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m tr_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/ganbert/lib/python3.10/site-packages/torch/_tensor.py:484\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    476\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    477\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 484\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ganbert/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve,average_precision_score\n",
    "\n",
    "stop_ext = EarlyStopper(is_loss = False,patience=4)\n",
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    transformer = AutoModel.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    hidden_size = int(config.hidden_size)\n",
    "    model = Multitask(symptom_num,emotion_num,transformer,input_size = hidden_size)\n",
    "    model = model.to(device)\n",
    "    train_df,test_df,symptoms_train,symptoms_test = build_dataset(df,df1)\n",
    "    train_dataloader = get_dataloader(train_df,tokenizer,symptoms_train,batch_size,do_shuffle=True)\n",
    "    train_model(model,train_dataloader,symptom_dataloader,classic_distill,classic_distill)\n",
    "    model.load_state_dict(torch.load(\"../models/temp\"))\n",
    "    \n",
    "    probs = []\n",
    "    for step,batch in enumerate(symptom_dataloader):\n",
    "        model.eval()\n",
    "        b_input_ids = batch[0].reshape(len(batch[0]),max_length).to(device)\n",
    "        b_attention_mask = batch[1].reshape(len(batch[0]),max_length).to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(b_input_ids,b_attention_mask)\n",
    "            for b_prob in (torch.nn.functional.sigmoid(outputs).detach().cpu()):\n",
    "                probs.append(b_prob)\n",
    "    probs = torch.stack(probs).numpy()\n",
    "    \n",
    "    preds = []\n",
    "    for i in range (probs.shape[1]):\n",
    "        precision,recall,thresholds = precision_recall_curve(test_labels.iloc[:,i],probs[:,i])\n",
    "        f1_scores = 2*recall*precision/(recall+precision)\n",
    "        cur_threshold = thresholds[np.nanargmax(f1_scores)]\n",
    "        print(\"melhor f1 para \",df.columns[i+1],\" \",np.nanmax(f1_scores))\n",
    "        preds.append(probs[:,i] >= cur_threshold)\n",
    "\n",
    "    \n",
    "    print(\"average precision score: \",average_precision_score(test_labels,probs))\n",
    "    cur_auc = average_precision_score(test_labels,probs)\n",
    "    if stop_ext.early_stop(cur_auc,save_best=True,model=model,model_path = \"../models/temp2\"):\n",
    "        break\n",
    "    df = get_logits(df,model)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganbert",
   "language": "python",
   "name": "ganbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
